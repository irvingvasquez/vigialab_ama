{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d700bf34",
   "metadata": {},
   "source": [
    "# Lenet classification in Mindspore for a custom dataset\n",
    "\n",
    "In this notebook we will implement a classification metodology using the mindspore framework.\n",
    "\n",
    "1. Define the required dataset. We will use a custom dataset.\n",
    "\n",
    "2. Define a network. The LeNet network is used in this example.\n",
    "\n",
    "3. Define the loss function and optimizer.\n",
    "\n",
    "4. Load dataset, perform training. After the training is complete, check the result and save the model file.\n",
    "\n",
    "5. Load the saved model for inference.\n",
    "\n",
    "6. Validate the model, load the test dataset and trained model, and validate the result accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95da2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import mindspore\n",
    "import argparse\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "from mindspore import context\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor\n",
    "from mindspore.train import Model\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd34b7d",
   "metadata": {},
   "source": [
    "## Define a dataset\n",
    "\n",
    "Mindspore implements *ImageFolderDataset*. A source dataset that reads images from a tree of directories. All images within one folder have the same label.\n",
    "\n",
    "The generated dataset has two columns: [image, label]. The tensor of column image is of the uint8 type. The tensor of column label is of a scalar of uint32 type.\n",
    "\n",
    ".\n",
    "└── image_folder_dataset_directory\n",
    "\n",
    "     ├── class1\n",
    "     │    ├── 000000000001.jpg\n",
    "     │    ├── 000000000002.jpg\n",
    "     │    ├── ...\n",
    "     ├── class2\n",
    "     │    ├── 000000000001.jpg\n",
    "     │    ├── 000000000002.jpg\n",
    "     │    ├── ...\n",
    "     ├── class3\n",
    "     │    ├── 000000000001.jpg\n",
    "     │    ├── 000000000002.jpg\n",
    "     │    ├── ...\n",
    "     ├── classN\n",
    "     ├── ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2011b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"D:\\Datasets\\cactus_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a1cd1",
   "metadata": {},
   "source": [
    "Defining the Dataset and Data Operations\n",
    "Define the create_dataset() function to create a dataset. In this function, define the data augmentation and processing operations to be performed.\n",
    "\n",
    "Define the dataset.\n",
    "\n",
    "Define parameters required for data augmentation and processing.\n",
    "\n",
    "Generate corresponding data augmentation operations according to the parameters.\n",
    "\n",
    "Use the map() mapping function to apply data operations to the dataset.\n",
    "\n",
    "Process the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145cfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    \"\"\" create dataset for train or test\n",
    "    Args:\n",
    "        data_path: Data path\n",
    "        batch_size: The number of data records in each group\n",
    "        repeat_size: The number of replicated data records\n",
    "        num_parallel_workers: The number of parallel workers\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    cactus_ds = ds.ImageFolderDataset(data_path, num_parallel_workers=num_parallel_workers, decode=True)\n",
    "\n",
    "    # define operation parameters\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # define map operations \n",
    "    # mindspore transformations\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)  # Resize images to (32, 32)\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml) # normalize images\n",
    "    rescale_op = CV.Rescale(rescale, shift) # rescale images\n",
    "    hwc2chw_op = CV.HWC2CHW() # change shape from (height, width, channel) to (channel, height, width) to fit network.\n",
    "    type_cast_op = C.TypeCast(mstype.int32) # change data type of label to int32 to fit network\n",
    "    \n",
    "    # apply map operations on images\n",
    "    cactus_ds = cactus_ds.map(input_columns=\"label\", operations=type_cast_op, num_parallel_workers=num_parallel_workers)\n",
    "    cactus_ds = cactus_ds.map(input_columns=\"image\", operations=resize_op, num_parallel_workers=num_parallel_workers)\n",
    "    cactus_ds = cactus_ds.map(input_columns=\"image\", operations=rescale_op, num_parallel_workers=num_parallel_workers)\n",
    "    cactus_ds = cactus_ds.map(input_columns=\"image\", operations=rescale_nml_op, num_parallel_workers=num_parallel_workers)\n",
    "    cactus_ds = cactus_ds.map(input_columns=\"image\", operations=hwc2chw_op, num_parallel_workers=num_parallel_workers)\n",
    "    \n",
    "    # apply DatasetOps\n",
    "    buffer_size = 1000\n",
    "    cactus_ds = cactus_ds.shuffle(buffer_size=buffer_size)\n",
    "    cactus_ds = cactus_ds.batch(batch_size, drop_remainder=True)\n",
    "    cactus_ds = cactus_ds.repeat(repeat_size)\n",
    "    \n",
    "    #NOTE, there is no \"Tensor\" definitions\n",
    "    return cactus_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df954f",
   "metadata": {},
   "source": [
    "## Implement neural network\n",
    "\n",
    "We will implement from scratch a Lenet5 architecture.\n",
    "\n",
    "![Lenet](./lenet5_cactus.PNG)\n",
    "\n",
    "Figure from: López-Jiménez, E., Vasquez-Gomez, J. I., Sanchez-Acevedo, M. A., Herrera-Lozada, J. C., & Uriarte-Arcia, A. V. (2019). Columnar cactus recognition in aerial images using a deep learning approach. Ecological Informatics, 52, 131-138."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b0e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variable\n",
    "def weight_variable():\n",
    "    \"\"\"\n",
    "    weight initial\n",
    "    \"\"\"\n",
    "    return TruncatedNormal(0.02)\n",
    "\n",
    "# Simplify the definition of the convolutions\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    conv layer weight initial\n",
    "    \"\"\"\n",
    "    weight = weight_variable()\n",
    "    return nn.Conv2d(in_channels, out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                     weight_init=weight, has_bias=False, pad_mode=\"valid\")\n",
    "\n",
    "# Define a fully connected layer\n",
    "def fc_with_initialize(input_channels, out_channels):\n",
    "    \"\"\"\n",
    "    fc layer weight initial\n",
    "    \"\"\"\n",
    "    weight = weight_variable()\n",
    "    bias = weight_variable()\n",
    "    return nn.Dense(input_channels, out_channels, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f597b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.ops.operations as P\n",
    "\n",
    "# Implementation of Lenet5\n",
    "class LeNet5(nn.Cell):\n",
    "    # nn.Cell. Base class for all neural networks.\n",
    "    \"\"\"\n",
    "    Lenet network structure\n",
    "    \"\"\"\n",
    "    #define the operators required\n",
    "    # output parameter set the number of classes\n",
    "    def __init__(self, output):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = conv(3, 6, 5)\n",
    "        self.conv2 = conv(6, 16, 5)\n",
    "        self.fc1 = fc_with_initialize(16 * 5 * 5, 120)\n",
    "        self.fc2 = fc_with_initialize(120, 84)\n",
    "        self.fc3 = fc_with_initialize(84, output)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.reshape = P.Reshape()\n",
    " \n",
    "    #use the preceding operators to construct networks\n",
    "    # Defines the computation to be performed.\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        batch_size, channels, _, _ = x.shape\n",
    "        x = self.reshape(x, (batch_size, -1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153c3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(args, model, epoch_size, path, repeat_size, ckpoint_cb, sink_mode, batch_size = 32):\n",
    "    \"\"\"Define the training method.\"\"\"\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    # load training dataset\n",
    "    ds_train = create_dataset(os.path.join(path, \"train\"), batch_size, repeat_size)\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor()], dataset_sink_mode=sink_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae0616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(args, network, model, path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    print(\"============== Starting Testing ==============\")\n",
    "    # load the saved model for evaluation\n",
    "    #param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
    "    # load parameter to the network\n",
    "    #load_param_into_net(network, param_dict)\n",
    "    # load testing dataset\n",
    "    ds_eval = create_dataset(os.path.join(path, \"test\"))\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
    "    print(\"============== Accuracy:{} ==============\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b8a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cadena = \"python test_mindspore --device_target\"\n",
    "local_device = \"CPU\"\n",
    "args = None\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=local_device)\n",
    "dataset_sink_mode = not local_device == \"CPU\"\n",
    "\n",
    "#define the loss function\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "epoch_size = 10\n",
    "repeat_size = epoch_size\n",
    "output = 2\n",
    "batch_size = 1024\n",
    "\n",
    "#create the network\n",
    "network = LeNet5(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ff2d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 0.6952155232429504\n",
      "epoch: 1 step: 2, loss is 0.69403076171875\n",
      "epoch: 1 step: 3, loss is 0.691548228263855\n",
      "epoch: 1 step: 4, loss is 0.6879672408103943\n",
      "epoch: 1 step: 5, loss is 0.6832858920097351\n",
      "epoch: 1 step: 6, loss is 0.6803513169288635\n",
      "epoch: 1 step: 7, loss is 0.6727116703987122\n",
      "epoch: 1 step: 8, loss is 0.6700455546379089\n",
      "epoch: 1 step: 9, loss is 0.6610921621322632\n",
      "epoch: 1 step: 10, loss is 0.6517544984817505\n",
      "epoch: 1 step: 11, loss is 0.6556546688079834\n",
      "epoch: 1 step: 12, loss is 0.6363601088523865\n",
      "epoch: 1 step: 13, loss is 0.6376200914382935\n",
      "epoch: 1 step: 14, loss is 0.6326032876968384\n",
      "epoch: 1 step: 15, loss is 0.6216421127319336\n",
      "epoch: 1 step: 16, loss is 0.6196250915527344\n",
      "epoch: 1 step: 17, loss is 0.6113178133964539\n",
      "epoch: 1 step: 18, loss is 0.619862973690033\n",
      "epoch: 1 step: 19, loss is 0.6075713038444519\n",
      "epoch: 1 step: 20, loss is 0.59392249584198\n",
      "epoch: 1 step: 21, loss is 0.5900442004203796\n",
      "epoch: 1 step: 22, loss is 0.5892221927642822\n",
      "epoch: 1 step: 23, loss is 0.5844918489456177\n",
      "epoch: 1 step: 24, loss is 0.5853393077850342\n",
      "epoch: 1 step: 25, loss is 0.5724116563796997\n",
      "epoch: 1 step: 26, loss is 0.586423397064209\n",
      "epoch: 1 step: 27, loss is 0.587770938873291\n",
      "epoch: 1 step: 28, loss is 0.5626925826072693\n",
      "epoch: 1 step: 29, loss is 0.5750327110290527\n",
      "epoch: 1 step: 30, loss is 0.5719005465507507\n",
      "epoch: 1 step: 31, loss is 0.5665242671966553\n",
      "epoch: 1 step: 32, loss is 0.5731194019317627\n",
      "epoch: 1 step: 33, loss is 0.5737156867980957\n",
      "epoch: 1 step: 34, loss is 0.5711731910705566\n",
      "epoch: 1 step: 35, loss is 0.5738369226455688\n",
      "epoch: 1 step: 36, loss is 0.552201509475708\n",
      "epoch: 1 step: 37, loss is 0.5298438668251038\n",
      "epoch: 1 step: 38, loss is 0.5604585409164429\n",
      "epoch: 1 step: 39, loss is 0.5784012079238892\n",
      "epoch: 1 step: 40, loss is 0.5772677659988403\n",
      "epoch: 1 step: 41, loss is 0.5551873445510864\n",
      "epoch: 1 step: 42, loss is 0.5683092474937439\n",
      "epoch: 1 step: 43, loss is 0.5730178356170654\n",
      "epoch: 1 step: 44, loss is 0.5630483031272888\n",
      "epoch: 1 step: 45, loss is 0.5808523297309875\n",
      "epoch: 1 step: 46, loss is 0.5567269921302795\n",
      "epoch: 1 step: 47, loss is 0.5646941065788269\n",
      "epoch: 1 step: 48, loss is 0.5656468868255615\n",
      "epoch: 1 step: 49, loss is 0.5511834025382996\n",
      "epoch: 1 step: 50, loss is 0.5531301498413086\n",
      "epoch: 1 step: 51, loss is 0.5686800479888916\n",
      "epoch: 1 step: 52, loss is 0.5487481355667114\n",
      "epoch: 1 step: 53, loss is 0.5634220242500305\n",
      "epoch: 1 step: 54, loss is 0.5697638988494873\n",
      "epoch: 1 step: 55, loss is 0.5399998426437378\n",
      "epoch: 1 step: 56, loss is 0.5730181336402893\n",
      "epoch: 1 step: 57, loss is 0.5483917593955994\n",
      "epoch: 1 step: 58, loss is 0.5666354298591614\n",
      "epoch: 1 step: 59, loss is 0.5418294072151184\n",
      "epoch: 1 step: 60, loss is 0.5688260793685913\n",
      "epoch: 1 step: 61, loss is 0.56451416015625\n",
      "epoch: 1 step: 62, loss is 0.5884529948234558\n",
      "epoch: 1 step: 63, loss is 0.5776050090789795\n",
      "epoch: 1 step: 64, loss is 0.5547242164611816\n",
      "epoch: 1 step: 65, loss is 0.5765504837036133\n",
      "epoch: 1 step: 66, loss is 0.5732868313789368\n",
      "epoch: 1 step: 67, loss is 0.5470659136772156\n",
      "epoch: 1 step: 68, loss is 0.5470629334449768\n",
      "epoch: 1 step: 69, loss is 0.5656499266624451\n",
      "epoch: 1 step: 70, loss is 0.5776893496513367\n",
      "epoch: 1 step: 71, loss is 0.5820595622062683\n",
      "epoch: 1 step: 72, loss is 0.5415863394737244\n",
      "epoch: 1 step: 73, loss is 0.5722139477729797\n",
      "epoch: 1 step: 74, loss is 0.5623634457588196\n",
      "epoch: 1 step: 75, loss is 0.5558051466941833\n",
      "epoch: 1 step: 76, loss is 0.5536229610443115\n",
      "epoch: 1 step: 77, loss is 0.5656370520591736\n",
      "epoch: 1 step: 78, loss is 0.5547187924385071\n",
      "epoch: 1 step: 79, loss is 0.542701780796051\n",
      "epoch: 1 step: 80, loss is 0.5623589754104614\n",
      "epoch: 1 step: 81, loss is 0.5874757766723633\n",
      "epoch: 1 step: 82, loss is 0.5689082145690918\n",
      "epoch: 1 step: 83, loss is 0.5220001339912415\n",
      "epoch: 1 step: 84, loss is 0.5601731538772583\n",
      "epoch: 1 step: 85, loss is 0.5732658505439758\n",
      "epoch: 1 step: 86, loss is 0.5525328516960144\n",
      "epoch: 1 step: 87, loss is 0.5667147040367126\n",
      "epoch: 1 step: 88, loss is 0.5950685143470764\n",
      "epoch: 1 step: 89, loss is 0.540558397769928\n",
      "epoch: 1 step: 90, loss is 0.5939265489578247\n",
      "epoch: 1 step: 91, loss is 0.5481999516487122\n",
      "epoch: 1 step: 92, loss is 0.5590823292732239\n",
      "epoch: 1 step: 93, loss is 0.5471268892288208\n",
      "epoch: 1 step: 94, loss is 0.5590808391571045\n",
      "epoch: 1 step: 95, loss is 0.5590803027153015\n",
      "epoch: 1 step: 96, loss is 0.5677687525749207\n",
      "epoch: 1 step: 97, loss is 0.5764426589012146\n",
      "epoch: 1 step: 98, loss is 0.5493268966674805\n",
      "epoch: 1 step: 99, loss is 0.531979501247406\n",
      "epoch: 1 step: 100, loss is 0.5720920562744141\n",
      "epoch: 1 step: 101, loss is 0.5569124221801758\n",
      "epoch: 1 step: 102, loss is 0.5677473545074463\n",
      "epoch: 1 step: 103, loss is 0.5590846538543701\n",
      "epoch: 1 step: 104, loss is 0.5493242144584656\n",
      "epoch: 1 step: 105, loss is 0.5569102764129639\n",
      "epoch: 1 step: 106, loss is 0.5547479391098022\n",
      "epoch: 1 step: 107, loss is 0.5742654800415039\n",
      "epoch: 1 step: 108, loss is 0.5666670799255371\n",
      "epoch: 1 step: 109, loss is 0.5623349547386169\n",
      "epoch: 1 step: 110, loss is 0.5785900354385376\n",
      "epoch: 1 step: 111, loss is 0.5569102168083191\n",
      "epoch: 1 step: 112, loss is 0.5774872899055481\n",
      "epoch: 1 step: 113, loss is 0.5612401366233826\n",
      "epoch: 1 step: 114, loss is 0.5363717079162598\n",
      "epoch: 1 step: 115, loss is 0.5796256065368652\n",
      "epoch: 1 step: 116, loss is 0.5655654668807983\n",
      "epoch: 1 step: 117, loss is 0.5688027143478394\n",
      "epoch: 1 step: 118, loss is 0.5439682006835938\n",
      "epoch: 1 step: 119, loss is 0.5558395385742188\n",
      "epoch: 1 step: 120, loss is 0.6033284664154053\n",
      "epoch: 1 step: 121, loss is 0.5731044411659241\n",
      "epoch: 1 step: 122, loss is 0.5590881705284119\n",
      "epoch: 1 step: 123, loss is 0.5580105781555176\n",
      "epoch: 1 step: 124, loss is 0.5451107025146484\n",
      "epoch: 1 step: 125, loss is 0.5451158881187439\n",
      "epoch: 1 step: 126, loss is 0.5773740410804749\n",
      "epoch: 1 step: 127, loss is 0.5580138564109802\n",
      "epoch: 1 step: 128, loss is 0.5504915714263916\n",
      "epoch: 1 step: 129, loss is 0.602084755897522\n",
      "epoch: 1 step: 130, loss is 0.5537237524986267\n",
      "epoch: 1 step: 131, loss is 0.5333269238471985\n",
      "epoch: 1 step: 132, loss is 0.5676819682121277\n",
      "epoch: 1 step: 133, loss is 0.5494274497032166\n",
      "epoch: 1 step: 134, loss is 0.5547990798950195\n",
      "epoch: 1 step: 135, loss is 0.5537217259407043\n",
      "epoch: 1 step: 136, loss is 0.5666198134422302\n",
      "epoch: 1 step: 137, loss is 0.5547904372215271\n",
      "epoch: 1 step: 138, loss is 0.5719929933547974\n",
      "epoch: 1 step: 139, loss is 0.5451040863990784\n",
      "epoch: 1 step: 140, loss is 0.5440199375152588\n",
      "epoch: 1 step: 141, loss is 0.5633968114852905\n",
      "epoch: 1 step: 142, loss is 0.5741779208183289\n",
      "epoch: 1 step: 143, loss is 0.5644768476486206\n",
      "epoch: 1 step: 144, loss is 0.5623171925544739\n",
      "epoch: 1 step: 145, loss is 0.5623096823692322\n",
      "epoch: 1 step: 146, loss is 0.6140840649604797\n",
      "epoch: 1 step: 147, loss is 0.5677056908607483\n",
      "epoch: 1 step: 148, loss is 0.5547823905944824\n",
      "epoch: 1 step: 149, loss is 0.5418638586997986\n",
      "epoch: 1 step: 150, loss is 0.5343377590179443\n",
      "epoch: 1 step: 151, loss is 0.5773814916610718\n",
      "epoch: 1 step: 152, loss is 0.5676959753036499\n",
      "epoch: 1 step: 153, loss is 0.5590847134590149\n",
      "epoch: 1 step: 154, loss is 0.5558521747589111\n",
      "epoch: 1 step: 155, loss is 0.5741550326347351\n",
      "epoch: 1 step: 156, loss is 0.558012843132019\n",
      "epoch: 1 step: 157, loss is 0.5418664216995239\n",
      "epoch: 1 step: 158, loss is 0.5687670707702637\n",
      "epoch: 1 step: 159, loss is 0.573082685470581\n",
      "epoch: 1 step: 160, loss is 0.5623141527175903\n",
      "epoch: 1 step: 161, loss is 0.5849031805992126\n",
      "epoch: 1 step: 162, loss is 0.5343562960624695\n",
      "epoch: 1 step: 163, loss is 0.5838217735290527\n",
      "epoch: 1 step: 164, loss is 0.5773603320121765\n",
      "epoch: 1 step: 165, loss is 0.554793119430542\n",
      "epoch: 1 step: 166, loss is 0.5236480236053467\n",
      "epoch: 1 step: 167, loss is 0.5558620095252991\n",
      "epoch: 1 step: 168, loss is 0.5547894239425659\n",
      "epoch: 1 step: 169, loss is 0.5881168246269226\n",
      "epoch: 1 step: 170, loss is 0.5590850114822388\n",
      "epoch: 2 step: 1, loss is 0.5730589032173157\n",
      "epoch: 2 step: 2, loss is 0.5698318481445312\n",
      "epoch: 2 step: 3, loss is 0.5483340620994568\n",
      "epoch: 2 step: 4, loss is 0.5795130729675293\n",
      "epoch: 2 step: 5, loss is 0.5773490071296692\n",
      "epoch: 2 step: 6, loss is 0.5612354874610901\n",
      "epoch: 2 step: 7, loss is 0.5483565926551819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 8, loss is 0.5408521294593811\n",
      "epoch: 2 step: 9, loss is 0.5676712393760681\n",
      "epoch: 2 step: 10, loss is 0.5698236227035522\n",
      "epoch: 2 step: 11, loss is 0.5558693408966064\n",
      "epoch: 2 step: 12, loss is 0.5666053891181946\n",
      "epoch: 2 step: 13, loss is 0.5451393723487854\n",
      "epoch: 2 step: 14, loss is 0.5590954422950745\n",
      "epoch: 2 step: 15, loss is 0.5644586682319641\n",
      "epoch: 2 step: 16, loss is 0.5569354295730591\n",
      "epoch: 2 step: 17, loss is 0.5676795244216919\n",
      "epoch: 2 step: 18, loss is 0.5483378767967224\n",
      "epoch: 2 step: 19, loss is 0.5666103959083557\n",
      "epoch: 2 step: 20, loss is 0.5666069984436035\n",
      "epoch: 2 step: 21, loss is 0.5526309609413147\n",
      "epoch: 2 step: 22, loss is 0.5666069984436035\n",
      "epoch: 2 step: 23, loss is 0.5719950795173645\n",
      "epoch: 2 step: 24, loss is 0.5698391795158386\n",
      "epoch: 2 step: 25, loss is 0.5666140913963318\n",
      "epoch: 2 step: 26, loss is 0.5472543239593506\n",
      "epoch: 2 step: 27, loss is 0.5870459675788879\n",
      "epoch: 2 step: 28, loss is 0.5676806569099426\n",
      "epoch: 2 step: 29, loss is 0.5537081360816956\n",
      "epoch: 2 step: 30, loss is 0.5494180917739868\n",
      "epoch: 2 step: 31, loss is 0.5580095052719116\n",
      "epoch: 2 step: 32, loss is 0.5494102835655212\n",
      "epoch: 2 step: 33, loss is 0.5751994848251343\n",
      "epoch: 2 step: 34, loss is 0.5569310784339905\n",
      "epoch: 2 step: 35, loss is 0.5569353699684143\n",
      "epoch: 2 step: 36, loss is 0.563383936882019\n",
      "epoch: 2 step: 37, loss is 0.5644591450691223\n",
      "epoch: 2 step: 38, loss is 0.5666148662567139\n",
      "epoch: 2 step: 39, loss is 0.5655295848846436\n",
      "epoch: 2 step: 40, loss is 0.5343522429466248\n",
      "epoch: 2 step: 41, loss is 0.580591082572937\n",
      "epoch: 2 step: 42, loss is 0.5633842945098877\n",
      "epoch: 2 step: 43, loss is 0.539712131023407\n",
      "epoch: 2 step: 44, loss is 0.5741413831710815\n",
      "epoch: 2 step: 45, loss is 0.5504674315452576\n",
      "epoch: 2 step: 46, loss is 0.5644634366035461\n",
      "epoch: 2 step: 47, loss is 0.5784521698951721\n",
      "epoch: 2 step: 48, loss is 0.581678569316864\n",
      "epoch: 2 step: 49, loss is 0.5386285781860352\n",
      "epoch: 2 step: 50, loss is 0.5666031241416931\n",
      "epoch: 2 step: 51, loss is 0.5579978823661804\n",
      "epoch: 2 step: 52, loss is 0.5892003178596497\n",
      "epoch: 2 step: 53, loss is 0.5504730939865112\n",
      "epoch: 2 step: 54, loss is 0.5666064620018005\n",
      "epoch: 2 step: 55, loss is 0.5784291625022888\n",
      "epoch: 2 step: 56, loss is 0.5547900795936584\n",
      "epoch: 2 step: 57, loss is 0.5676760673522949\n",
      "epoch: 2 step: 58, loss is 0.5826928019523621\n",
      "epoch: 2 step: 59, loss is 0.5655272603034973\n",
      "epoch: 2 step: 60, loss is 0.5601540803909302\n",
      "epoch: 2 step: 61, loss is 0.5633741617202759\n",
      "epoch: 2 step: 62, loss is 0.5676512718200684\n",
      "epoch: 2 step: 63, loss is 0.5451822876930237\n",
      "epoch: 2 step: 64, loss is 0.551600456237793\n",
      "epoch: 2 step: 65, loss is 0.5601629614830017\n",
      "epoch: 2 step: 66, loss is 0.5473222136497498\n",
      "epoch: 2 step: 67, loss is 0.5462487936019897\n",
      "epoch: 2 step: 68, loss is 0.5483811497688293\n",
      "epoch: 2 step: 69, loss is 0.5837349891662598\n",
      "epoch: 2 step: 70, loss is 0.5312169194221497\n",
      "epoch: 2 step: 71, loss is 0.5655220150947571\n",
      "epoch: 2 step: 72, loss is 0.5526338219642639\n",
      "epoch: 2 step: 73, loss is 0.5451056957244873\n",
      "epoch: 2 step: 74, loss is 0.576273500919342\n",
      "epoch: 2 step: 75, loss is 0.5633760094642639\n",
      "epoch: 2 step: 76, loss is 0.5676802396774292\n",
      "epoch: 2 step: 77, loss is 0.5558403730392456\n",
      "epoch: 2 step: 78, loss is 0.5623019337654114\n",
      "epoch: 2 step: 79, loss is 0.559075653553009\n",
      "epoch: 2 step: 80, loss is 0.5601406693458557\n",
      "epoch: 2 step: 81, loss is 0.5601415634155273\n",
      "epoch: 2 step: 82, loss is 0.5471962690353394\n",
      "epoch: 2 step: 83, loss is 0.5828186869621277\n",
      "epoch: 2 step: 84, loss is 0.5515020489692688\n",
      "epoch: 2 step: 85, loss is 0.586074709892273\n",
      "epoch: 2 step: 86, loss is 0.582819402217865\n",
      "epoch: 2 step: 87, loss is 0.5612195134162903\n",
      "epoch: 2 step: 88, loss is 0.5666233897209167\n",
      "epoch: 2 step: 89, loss is 0.5622938275337219\n",
      "epoch: 2 step: 90, loss is 0.5536800622940063\n",
      "epoch: 2 step: 91, loss is 0.5536806583404541\n",
      "epoch: 2 step: 92, loss is 0.5709130764007568\n",
      "epoch: 2 step: 93, loss is 0.5515302419662476\n",
      "epoch: 2 step: 94, loss is 0.5472216606140137\n",
      "epoch: 2 step: 95, loss is 0.5687631964683533\n",
      "epoch: 2 step: 96, loss is 0.5623027682304382\n",
      "epoch: 2 step: 97, loss is 0.5493783950805664\n",
      "epoch: 2 step: 98, loss is 0.5762961506843567\n",
      "epoch: 2 step: 99, loss is 0.5881426334381104\n",
      "epoch: 2 step: 100, loss is 0.5461475253105164\n",
      "epoch: 2 step: 101, loss is 0.5353910326957703\n",
      "epoch: 2 step: 102, loss is 0.5730593204498291\n",
      "epoch: 2 step: 103, loss is 0.5719897747039795\n",
      "epoch: 2 step: 104, loss is 0.5784384608268738\n",
      "epoch: 2 step: 105, loss is 0.5526131987571716\n",
      "epoch: 2 step: 106, loss is 0.5353969931602478\n",
      "epoch: 2 step: 107, loss is 0.5709033608436584\n",
      "epoch: 2 step: 108, loss is 0.5816644430160522\n",
      "epoch: 2 step: 109, loss is 0.5795037150382996\n",
      "epoch: 2 step: 110, loss is 0.5837963819503784\n",
      "epoch: 2 step: 111, loss is 0.5386586785316467\n",
      "epoch: 2 step: 112, loss is 0.5676562786102295\n",
      "epoch: 2 step: 113, loss is 0.5440492033958435\n",
      "epoch: 2 step: 114, loss is 0.5386833548545837\n",
      "epoch: 2 step: 115, loss is 0.5526339411735535\n",
      "epoch: 2 step: 116, loss is 0.5622931718826294\n",
      "epoch: 2 step: 117, loss is 0.5504783987998962\n",
      "epoch: 2 step: 118, loss is 0.569811999797821\n",
      "epoch: 2 step: 119, loss is 0.5795056223869324\n",
      "epoch: 2 step: 120, loss is 0.5633674263954163\n",
      "epoch: 2 step: 121, loss is 0.5644356608390808\n",
      "epoch: 2 step: 122, loss is 0.54616379737854\n",
      "epoch: 2 step: 123, loss is 0.5504528880119324\n",
      "epoch: 2 step: 124, loss is 0.5698229670524597\n",
      "epoch: 2 step: 125, loss is 0.5461487174034119\n",
      "epoch: 2 step: 126, loss is 0.556905210018158\n",
      "epoch: 2 step: 127, loss is 0.590302586555481\n",
      "epoch: 2 step: 128, loss is 0.5881423354148865\n",
      "epoch: 2 step: 129, loss is 0.5590655207633972\n",
      "epoch: 2 step: 130, loss is 0.5644412636756897\n",
      "epoch: 2 step: 131, loss is 0.5504549145698547\n",
      "epoch: 2 step: 132, loss is 0.5752016305923462\n",
      "epoch: 2 step: 133, loss is 0.5579864978790283\n",
      "epoch: 2 step: 134, loss is 0.5590691566467285\n",
      "epoch: 2 step: 135, loss is 0.5483232736587524\n",
      "epoch: 2 step: 136, loss is 0.5612204670906067\n",
      "epoch: 2 step: 137, loss is 0.5397173762321472\n",
      "epoch: 2 step: 138, loss is 0.564439058303833\n",
      "epoch: 2 step: 139, loss is 0.561217188835144\n",
      "epoch: 2 step: 140, loss is 0.532161295413971\n",
      "epoch: 2 step: 141, loss is 0.5816736221313477\n",
      "epoch: 2 step: 142, loss is 0.5784498453140259\n",
      "epoch: 2 step: 143, loss is 0.5181229710578918\n",
      "epoch: 2 step: 144, loss is 0.5719882249832153\n",
      "epoch: 2 step: 145, loss is 0.5633718371391296\n",
      "epoch: 2 step: 146, loss is 0.5914126634597778\n",
      "epoch: 2 step: 147, loss is 0.5892567038536072\n",
      "epoch: 2 step: 148, loss is 0.5644494295120239\n",
      "epoch: 2 step: 149, loss is 0.5806085467338562\n",
      "epoch: 2 step: 150, loss is 0.545062243938446\n",
      "epoch: 2 step: 151, loss is 0.5665915012359619\n",
      "epoch: 2 step: 152, loss is 0.5407777428627014\n",
      "epoch: 2 step: 153, loss is 0.5504568815231323\n",
      "epoch: 2 step: 154, loss is 0.5665861964225769\n",
      "epoch: 2 step: 155, loss is 0.5601388216018677\n",
      "epoch: 2 step: 156, loss is 0.5870128273963928\n",
      "epoch: 2 step: 157, loss is 0.551537275314331\n",
      "epoch: 2 step: 158, loss is 0.5622854828834534\n",
      "epoch: 2 step: 159, loss is 0.5547689199447632\n",
      "epoch: 2 step: 160, loss is 0.5569084882736206\n",
      "epoch: 2 step: 161, loss is 0.5558457970619202\n",
      "epoch: 2 step: 162, loss is 0.588079035282135\n",
      "epoch: 2 step: 163, loss is 0.5612072944641113\n",
      "epoch: 2 step: 164, loss is 0.5655040740966797\n",
      "epoch: 2 step: 165, loss is 0.5408066511154175\n",
      "epoch: 2 step: 166, loss is 0.5676423907279968\n",
      "epoch: 2 step: 167, loss is 0.5579873919487\n",
      "epoch: 2 step: 168, loss is 0.5590500235557556\n",
      "epoch: 2 step: 169, loss is 0.5590628385543823\n",
      "epoch: 2 step: 170, loss is 0.5547640919685364\n",
      "epoch: 3 step: 1, loss is 0.5665799975395203\n",
      "epoch: 3 step: 2, loss is 0.560139000415802\n",
      "epoch: 3 step: 3, loss is 0.5407830476760864\n",
      "epoch: 3 step: 4, loss is 0.5913118124008179\n",
      "epoch: 3 step: 5, loss is 0.5493766069412231\n",
      "epoch: 3 step: 6, loss is 0.5999221801757812\n",
      "epoch: 3 step: 7, loss is 0.5622794032096863\n",
      "epoch: 3 step: 8, loss is 0.5483178496360779\n",
      "epoch: 3 step: 9, loss is 0.5633595585823059\n",
      "epoch: 3 step: 10, loss is 0.5794658660888672\n",
      "epoch: 3 step: 11, loss is 0.5773071050643921\n",
      "epoch: 3 step: 12, loss is 0.525801420211792\n",
      "epoch: 3 step: 13, loss is 0.5526218414306641\n",
      "epoch: 3 step: 14, loss is 0.5687225461006165\n",
      "epoch: 3 step: 15, loss is 0.5440276265144348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 16, loss is 0.5611999034881592\n",
      "epoch: 3 step: 17, loss is 0.5644224286079407\n",
      "epoch: 3 step: 18, loss is 0.5429418683052063\n",
      "epoch: 3 step: 19, loss is 0.5719524025917053\n",
      "epoch: 3 step: 20, loss is 0.5429210662841797\n",
      "epoch: 3 step: 21, loss is 0.5741102695465088\n",
      "epoch: 3 step: 22, loss is 0.550438404083252\n",
      "epoch: 3 step: 23, loss is 0.5859699249267578\n",
      "epoch: 3 step: 24, loss is 0.5611977577209473\n",
      "epoch: 3 step: 25, loss is 0.5310482382774353\n",
      "epoch: 3 step: 26, loss is 0.5676606297492981\n",
      "epoch: 3 step: 27, loss is 0.5428804159164429\n",
      "epoch: 3 step: 28, loss is 0.5644246935844421\n",
      "epoch: 3 step: 29, loss is 0.5720024704933167\n",
      "epoch: 3 step: 30, loss is 0.562275767326355\n",
      "epoch: 3 step: 31, loss is 0.5849499106407166\n",
      "epoch: 3 step: 32, loss is 0.5514843463897705\n",
      "epoch: 3 step: 33, loss is 0.5547221302986145\n",
      "epoch: 3 step: 34, loss is 0.5871152877807617\n",
      "epoch: 3 step: 35, loss is 0.55256187915802\n",
      "epoch: 3 step: 36, loss is 0.5450081825256348\n",
      "epoch: 3 step: 37, loss is 0.5870974063873291\n",
      "epoch: 3 step: 38, loss is 0.5417839884757996\n",
      "epoch: 3 step: 39, loss is 0.5698338150978088\n",
      "epoch: 3 step: 40, loss is 0.5881563425064087\n",
      "epoch: 3 step: 41, loss is 0.5353241562843323\n",
      "epoch: 3 step: 42, loss is 0.5665855407714844\n",
      "epoch: 3 step: 43, loss is 0.5698171257972717\n",
      "epoch: 3 step: 44, loss is 0.5461119413375854\n",
      "epoch: 3 step: 45, loss is 0.5558136105537415\n",
      "epoch: 3 step: 46, loss is 0.5719742178916931\n",
      "epoch: 3 step: 47, loss is 0.5601092576980591\n",
      "epoch: 3 step: 48, loss is 0.5687344074249268\n",
      "epoch: 3 step: 49, loss is 0.5611907839775085\n",
      "epoch: 3 step: 50, loss is 0.5558111667633057\n",
      "epoch: 3 step: 51, loss is 0.5601154565811157\n",
      "epoch: 3 step: 52, loss is 0.5482717156410217\n",
      "epoch: 3 step: 53, loss is 0.585972011089325\n",
      "epoch: 3 step: 54, loss is 0.5536503195762634\n",
      "epoch: 3 step: 55, loss is 0.5795003771781921\n",
      "epoch: 3 step: 56, loss is 0.5611881613731384\n",
      "epoch: 3 step: 57, loss is 0.5880903601646423\n",
      "epoch: 3 step: 58, loss is 0.5601069331169128\n",
      "epoch: 3 step: 59, loss is 0.5773080587387085\n",
      "epoch: 3 step: 60, loss is 0.5407975316047668\n",
      "epoch: 3 step: 61, loss is 0.5697858333587646\n",
      "epoch: 3 step: 62, loss is 0.5579703450202942\n",
      "epoch: 3 step: 63, loss is 0.5826386213302612\n",
      "epoch: 3 step: 64, loss is 0.5419155359268188\n",
      "epoch: 3 step: 65, loss is 0.5611901879310608\n",
      "epoch: 3 step: 66, loss is 0.5622628927230835\n",
      "epoch: 3 step: 67, loss is 0.5279991030693054\n",
      "epoch: 3 step: 68, loss is 0.5537001490592957\n",
      "epoch: 3 step: 69, loss is 0.5686992406845093\n",
      "epoch: 3 step: 70, loss is 0.55690598487854\n",
      "epoch: 3 step: 71, loss is 0.5354498028755188\n",
      "epoch: 3 step: 72, loss is 0.5762238502502441\n",
      "epoch: 3 step: 73, loss is 0.5568891167640686\n",
      "epoch: 3 step: 74, loss is 0.5547528266906738\n",
      "epoch: 3 step: 75, loss is 0.5364469289779663\n",
      "epoch: 3 step: 76, loss is 0.5665623545646667\n",
      "epoch: 3 step: 77, loss is 0.5514859557151794\n",
      "epoch: 3 step: 78, loss is 0.5471561551094055\n",
      "epoch: 3 step: 79, loss is 0.5773769021034241\n",
      "epoch: 3 step: 80, loss is 0.5741427540779114\n",
      "epoch: 3 step: 81, loss is 0.5784714818000793\n",
      "epoch: 3 step: 82, loss is 0.5655105710029602\n",
      "epoch: 3 step: 83, loss is 0.586046576499939\n",
      "epoch: 3 step: 84, loss is 0.5492964386940002\n",
      "epoch: 3 step: 85, loss is 0.5622743368148804\n",
      "epoch: 3 step: 86, loss is 0.5881958603858948\n",
      "epoch: 3 step: 87, loss is 0.556865930557251\n",
      "epoch: 3 step: 88, loss is 0.5460852980613708\n",
      "epoch: 3 step: 89, loss is 0.5471661686897278\n",
      "epoch: 3 step: 90, loss is 0.5460927486419678\n",
      "epoch: 3 step: 91, loss is 0.5503918528556824\n",
      "epoch: 3 step: 92, loss is 0.571963906288147\n",
      "epoch: 3 step: 93, loss is 0.5838335156440735\n",
      "epoch: 3 step: 94, loss is 0.5579491853713989\n",
      "epoch: 3 step: 95, loss is 0.565505862236023\n",
      "epoch: 3 step: 96, loss is 0.5514729619026184\n",
      "epoch: 3 step: 97, loss is 0.5525590181350708\n",
      "epoch: 3 step: 98, loss is 0.5633336305618286\n",
      "epoch: 3 step: 99, loss is 0.5913776159286499\n",
      "epoch: 3 step: 100, loss is 0.5536237359046936\n",
      "epoch: 3 step: 101, loss is 0.5417702198028564\n",
      "epoch: 3 step: 102, loss is 0.5751897692680359\n",
      "epoch: 3 step: 103, loss is 0.5708792209625244\n",
      "epoch: 3 step: 104, loss is 0.5697913765907288\n",
      "epoch: 3 step: 105, loss is 0.5708748698234558\n",
      "epoch: 3 step: 106, loss is 0.5687263607978821\n",
      "epoch: 3 step: 107, loss is 0.5622560977935791\n",
      "epoch: 3 step: 108, loss is 0.5740717649459839\n",
      "epoch: 3 step: 109, loss is 0.5676183700561523\n",
      "epoch: 3 step: 110, loss is 0.5311186909675598\n",
      "epoch: 3 step: 111, loss is 0.5439964532852173\n",
      "epoch: 3 step: 112, loss is 0.571904182434082\n",
      "epoch: 3 step: 113, loss is 0.5601096153259277\n",
      "epoch: 3 step: 114, loss is 0.5772889852523804\n",
      "epoch: 3 step: 115, loss is 0.5654632449150085\n",
      "epoch: 3 step: 116, loss is 0.5568851232528687\n",
      "epoch: 3 step: 117, loss is 0.5311349034309387\n",
      "epoch: 3 step: 118, loss is 0.5643868446350098\n",
      "epoch: 3 step: 119, loss is 0.5601037740707397\n",
      "epoch: 3 step: 120, loss is 0.5708378553390503\n",
      "epoch: 3 step: 121, loss is 0.5418365001678467\n",
      "epoch: 3 step: 122, loss is 0.5762138962745667\n",
      "epoch: 3 step: 123, loss is 0.5611792206764221\n",
      "epoch: 3 step: 124, loss is 0.5676162242889404\n",
      "epoch: 3 step: 125, loss is 0.5719245672225952\n",
      "epoch: 3 step: 126, loss is 0.5665479898452759\n",
      "epoch: 3 step: 127, loss is 0.5525668263435364\n",
      "epoch: 3 step: 128, loss is 0.5643881559371948\n",
      "epoch: 3 step: 129, loss is 0.5482701063156128\n",
      "epoch: 3 step: 130, loss is 0.5686996579170227\n",
      "epoch: 3 step: 131, loss is 0.5697737336158752\n",
      "epoch: 3 step: 132, loss is 0.5407370924949646\n",
      "epoch: 3 step: 133, loss is 0.5525761842727661\n",
      "epoch: 3 step: 134, loss is 0.5643952488899231\n",
      "epoch: 3 step: 135, loss is 0.5719142556190491\n",
      "epoch: 3 step: 136, loss is 0.5536360740661621\n",
      "epoch: 3 step: 137, loss is 0.5794645547866821\n",
      "epoch: 3 step: 138, loss is 0.5590203404426575\n",
      "epoch: 3 step: 139, loss is 0.5482534766197205\n",
      "epoch: 3 step: 140, loss is 0.5611616373062134\n",
      "epoch: 3 step: 141, loss is 0.5697674751281738\n",
      "epoch: 3 step: 142, loss is 0.5514675974845886\n",
      "epoch: 3 step: 143, loss is 0.5633260011672974\n",
      "epoch: 3 step: 144, loss is 0.5482286214828491\n",
      "epoch: 3 step: 145, loss is 0.5740854144096375\n",
      "epoch: 3 step: 146, loss is 0.577309250831604\n",
      "epoch: 3 step: 147, loss is 0.5417706370353699\n",
      "epoch: 3 step: 148, loss is 0.5514608025550842\n",
      "epoch: 3 step: 149, loss is 0.5611587762832642\n",
      "epoch: 3 step: 150, loss is 0.5536121726036072\n",
      "epoch: 3 step: 151, loss is 0.5417523980140686\n",
      "epoch: 3 step: 152, loss is 0.5816676020622253\n",
      "epoch: 3 step: 153, loss is 0.5838311314582825\n",
      "epoch: 3 step: 154, loss is 0.5708773732185364\n",
      "epoch: 3 step: 155, loss is 0.5611535906791687\n",
      "epoch: 3 step: 156, loss is 0.5794961452484131\n",
      "epoch: 3 step: 157, loss is 0.5891839861869812\n",
      "epoch: 3 step: 158, loss is 0.5697849988937378\n",
      "epoch: 3 step: 159, loss is 0.5654638409614563\n",
      "epoch: 3 step: 160, loss is 0.5299903154373169\n",
      "epoch: 3 step: 161, loss is 0.54288649559021\n",
      "epoch: 3 step: 162, loss is 0.5729632377624512\n",
      "epoch: 3 step: 163, loss is 0.570810079574585\n",
      "epoch: 3 step: 164, loss is 0.5729628801345825\n",
      "epoch: 3 step: 165, loss is 0.564372718334198\n",
      "epoch: 3 step: 166, loss is 0.5279108285903931\n",
      "epoch: 3 step: 167, loss is 0.5622278451919556\n",
      "epoch: 3 step: 168, loss is 0.5718730092048645\n",
      "epoch: 3 step: 169, loss is 0.5514885187149048\n",
      "epoch: 3 step: 170, loss is 0.5375327467918396\n",
      "epoch: 4 step: 1, loss is 0.5654562711715698\n",
      "epoch: 4 step: 2, loss is 0.5450193881988525\n",
      "epoch: 4 step: 3, loss is 0.5697513818740845\n",
      "epoch: 4 step: 4, loss is 0.555763304233551\n",
      "epoch: 4 step: 5, loss is 0.5471685528755188\n",
      "epoch: 4 step: 6, loss is 0.564373254776001\n",
      "epoch: 4 step: 7, loss is 0.5697553753852844\n",
      "epoch: 4 step: 8, loss is 0.5481925010681152\n",
      "epoch: 4 step: 9, loss is 0.5579118728637695\n",
      "epoch: 4 step: 10, loss is 0.564385175704956\n",
      "epoch: 4 step: 11, loss is 0.5795028209686279\n",
      "epoch: 4 step: 12, loss is 0.542776346206665\n",
      "epoch: 4 step: 13, loss is 0.5708617568016052\n",
      "epoch: 4 step: 14, loss is 0.5708659291267395\n",
      "epoch: 4 step: 15, loss is 0.5622084736824036\n",
      "epoch: 4 step: 16, loss is 0.5579061508178711\n",
      "epoch: 4 step: 17, loss is 0.5665456056594849\n",
      "epoch: 4 step: 18, loss is 0.5514042973518372\n",
      "epoch: 4 step: 19, loss is 0.5892248153686523\n",
      "epoch: 4 step: 20, loss is 0.5816523432731628\n",
      "epoch: 4 step: 21, loss is 0.5665402412414551\n",
      "epoch: 4 step: 22, loss is 0.5428077578544617\n",
      "epoch: 4 step: 23, loss is 0.561130702495575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 24, loss is 0.5611310601234436\n",
      "epoch: 4 step: 25, loss is 0.5632880330085754\n",
      "epoch: 4 step: 26, loss is 0.542823851108551\n",
      "epoch: 4 step: 27, loss is 0.5772746205329895\n",
      "epoch: 4 step: 28, loss is 0.5622066855430603\n",
      "epoch: 4 step: 29, loss is 0.5589843392372131\n",
      "epoch: 4 step: 30, loss is 0.5643534064292908\n",
      "epoch: 4 step: 31, loss is 0.5482044219970703\n",
      "epoch: 4 step: 32, loss is 0.5546740293502808\n",
      "epoch: 4 step: 33, loss is 0.5514424443244934\n",
      "epoch: 4 step: 34, loss is 0.5718919634819031\n",
      "epoch: 4 step: 35, loss is 0.5428271889686584\n",
      "epoch: 4 step: 36, loss is 0.5600424408912659\n",
      "epoch: 4 step: 37, loss is 0.5643560290336609\n",
      "epoch: 4 step: 38, loss is 0.5557403564453125\n",
      "epoch: 4 step: 39, loss is 0.5654335021972656\n",
      "epoch: 4 step: 40, loss is 0.558964729309082\n",
      "epoch: 4 step: 41, loss is 0.5632789731025696\n",
      "epoch: 4 step: 42, loss is 0.5524916052818298\n",
      "epoch: 4 step: 43, loss is 0.5427651405334473\n",
      "epoch: 4 step: 44, loss is 0.5546456575393677\n",
      "epoch: 4 step: 45, loss is 0.5740681290626526\n",
      "epoch: 4 step: 46, loss is 0.5470588803291321\n",
      "epoch: 4 step: 47, loss is 0.5470678806304932\n",
      "epoch: 4 step: 48, loss is 0.6152344942092896\n",
      "epoch: 4 step: 49, loss is 0.5816667675971985\n",
      "epoch: 4 step: 50, loss is 0.554625391960144\n",
      "epoch: 4 step: 51, loss is 0.5686821937561035\n",
      "epoch: 4 step: 52, loss is 0.5449191331863403\n",
      "epoch: 4 step: 53, loss is 0.5492390394210815\n",
      "epoch: 4 step: 54, loss is 0.5719033479690552\n",
      "epoch: 4 step: 55, loss is 0.5567912459373474\n",
      "epoch: 4 step: 56, loss is 0.5567834377288818\n",
      "epoch: 4 step: 57, loss is 0.5686660408973694\n",
      "epoch: 4 step: 58, loss is 0.5621755123138428\n",
      "epoch: 4 step: 59, loss is 0.5697290897369385\n",
      "epoch: 4 step: 60, loss is 0.5481590628623962\n",
      "epoch: 4 step: 61, loss is 0.5664956569671631\n",
      "epoch: 4 step: 62, loss is 0.5729770064353943\n",
      "epoch: 4 step: 63, loss is 0.530903697013855\n",
      "epoch: 4 step: 64, loss is 0.5611056089401245\n",
      "epoch: 4 step: 65, loss is 0.5610998272895813\n",
      "epoch: 4 step: 66, loss is 0.5815943479537964\n",
      "epoch: 4 step: 67, loss is 0.5751262903213501\n",
      "epoch: 4 step: 68, loss is 0.5654102563858032\n",
      "epoch: 4 step: 69, loss is 0.5632383823394775\n",
      "epoch: 4 step: 70, loss is 0.5460125803947449\n",
      "epoch: 4 step: 71, loss is 0.5589393973350525\n",
      "epoch: 4 step: 72, loss is 0.5621805787086487\n",
      "epoch: 4 step: 73, loss is 0.556781530380249\n",
      "epoch: 4 step: 74, loss is 0.5729349851608276\n",
      "epoch: 4 step: 75, loss is 0.5632491707801819\n",
      "epoch: 4 step: 76, loss is 0.5879943370819092\n",
      "epoch: 4 step: 77, loss is 0.5438767671585083\n",
      "epoch: 4 step: 78, loss is 0.56539386510849\n",
      "epoch: 4 step: 79, loss is 0.5643090009689331\n",
      "epoch: 4 step: 80, loss is 0.5439063906669617\n",
      "epoch: 4 step: 81, loss is 0.5643104314804077\n",
      "epoch: 4 step: 82, loss is 0.5643122792243958\n",
      "epoch: 4 step: 83, loss is 0.5610864162445068\n",
      "epoch: 4 step: 84, loss is 0.5686008334159851\n",
      "epoch: 4 step: 85, loss is 0.5535607933998108\n",
      "epoch: 4 step: 86, loss is 0.5696679949760437\n",
      "epoch: 4 step: 87, loss is 0.5825607180595398\n",
      "epoch: 4 step: 88, loss is 0.5782630443572998\n",
      "epoch: 4 step: 89, loss is 0.555719792842865\n",
      "epoch: 4 step: 90, loss is 0.5750207304954529\n",
      "epoch: 4 step: 91, loss is 0.5428494811058044\n",
      "epoch: 4 step: 92, loss is 0.5450142621994019\n",
      "epoch: 4 step: 93, loss is 0.557875394821167\n",
      "epoch: 4 step: 94, loss is 0.5653530955314636\n",
      "epoch: 4 step: 95, loss is 0.5685886144638062\n",
      "epoch: 4 step: 96, loss is 0.5407294631004333\n",
      "epoch: 4 step: 97, loss is 0.569634199142456\n",
      "epoch: 4 step: 98, loss is 0.5600062608718872\n",
      "epoch: 4 step: 99, loss is 0.5546373724937439\n",
      "epoch: 4 step: 100, loss is 0.5846463441848755\n",
      "epoch: 4 step: 101, loss is 0.5503348112106323\n",
      "epoch: 4 step: 102, loss is 0.5353426337242126\n",
      "epoch: 4 step: 103, loss is 0.5621334314346313\n",
      "epoch: 4 step: 104, loss is 0.53960120677948\n",
      "epoch: 4 step: 105, loss is 0.578262209892273\n",
      "epoch: 4 step: 106, loss is 0.5481573939323425\n",
      "epoch: 4 step: 107, loss is 0.5621264576911926\n",
      "epoch: 4 step: 108, loss is 0.5653524398803711\n",
      "epoch: 4 step: 109, loss is 0.5524436831474304\n",
      "epoch: 4 step: 110, loss is 0.5664471387863159\n",
      "epoch: 4 step: 111, loss is 0.5664280652999878\n",
      "epoch: 4 step: 112, loss is 0.569660484790802\n",
      "epoch: 4 step: 113, loss is 0.5664397478103638\n",
      "epoch: 4 step: 114, loss is 0.5405719876289368\n",
      "epoch: 4 step: 115, loss is 0.544873833656311\n",
      "epoch: 4 step: 116, loss is 0.567513644695282\n",
      "epoch: 4 step: 117, loss is 0.5847948789596558\n",
      "epoch: 4 step: 118, loss is 0.5545821189880371\n",
      "epoch: 4 step: 119, loss is 0.5675265789031982\n",
      "epoch: 4 step: 120, loss is 0.54807448387146\n",
      "epoch: 4 step: 121, loss is 0.5405171513557434\n",
      "epoch: 4 step: 122, loss is 0.5523998737335205\n",
      "epoch: 4 step: 123, loss is 0.5588732957839966\n",
      "epoch: 4 step: 124, loss is 0.5794210433959961\n",
      "epoch: 4 step: 125, loss is 0.5534694194793701\n",
      "epoch: 4 step: 126, loss is 0.576193630695343\n",
      "epoch: 4 step: 127, loss is 0.5751050710678101\n",
      "epoch: 4 step: 128, loss is 0.5545408725738525\n",
      "epoch: 4 step: 129, loss is 0.5339875221252441\n",
      "epoch: 4 step: 130, loss is 0.5545448064804077\n",
      "epoch: 4 step: 131, loss is 0.5653442740440369\n",
      "epoch: 4 step: 132, loss is 0.5761851668357849\n",
      "epoch: 4 step: 133, loss is 0.5685850381851196\n",
      "epoch: 4 step: 134, loss is 0.5696802139282227\n",
      "epoch: 4 step: 135, loss is 0.5621158480644226\n",
      "epoch: 4 step: 136, loss is 0.5718570351600647\n",
      "epoch: 4 step: 137, loss is 0.5729098916053772\n",
      "epoch: 4 step: 138, loss is 0.5566937923431396\n",
      "epoch: 4 step: 139, loss is 0.5977396368980408\n",
      "epoch: 4 step: 140, loss is 0.5416046977043152\n",
      "epoch: 4 step: 141, loss is 0.5351592898368835\n",
      "epoch: 4 step: 142, loss is 0.5717734098434448\n",
      "epoch: 4 step: 143, loss is 0.5513206124305725\n",
      "epoch: 4 step: 144, loss is 0.5459333658218384\n",
      "epoch: 4 step: 145, loss is 0.5437502861022949\n",
      "epoch: 4 step: 146, loss is 0.5685370564460754\n",
      "epoch: 4 step: 147, loss is 0.5502341985702515\n",
      "epoch: 4 step: 148, loss is 0.5728532075881958\n",
      "epoch: 4 step: 149, loss is 0.5825786590576172\n",
      "epoch: 4 step: 150, loss is 0.5566974878311157\n",
      "epoch: 4 step: 151, loss is 0.5749998092651367\n",
      "epoch: 4 step: 152, loss is 0.5642327666282654\n",
      "epoch: 4 step: 153, loss is 0.5631330013275146\n",
      "epoch: 4 step: 154, loss is 0.5685284733772278\n",
      "epoch: 4 step: 155, loss is 0.5739040970802307\n",
      "epoch: 4 step: 156, loss is 0.5459147691726685\n",
      "epoch: 4 step: 157, loss is 0.5620698928833008\n",
      "epoch: 4 step: 158, loss is 0.535181999206543\n",
      "epoch: 4 step: 159, loss is 0.5556038618087769\n",
      "epoch: 4 step: 160, loss is 0.5437678098678589\n",
      "epoch: 4 step: 161, loss is 0.5835694074630737\n",
      "epoch: 4 step: 162, loss is 0.5771154165267944\n",
      "epoch: 4 step: 163, loss is 0.5739015340805054\n",
      "epoch: 4 step: 164, loss is 0.5545163154602051\n",
      "epoch: 4 step: 165, loss is 0.5867934823036194\n",
      "epoch: 4 step: 166, loss is 0.5513122081756592\n",
      "epoch: 4 step: 167, loss is 0.5351682901382446\n",
      "epoch: 4 step: 168, loss is 0.5749514698982239\n",
      "epoch: 4 step: 169, loss is 0.5620439052581787\n",
      "epoch: 4 step: 170, loss is 0.5609706044197083\n",
      "epoch: 5 step: 1, loss is 0.580285370349884\n",
      "epoch: 5 step: 2, loss is 0.5287368297576904\n",
      "epoch: 5 step: 3, loss is 0.557744026184082\n",
      "epoch: 5 step: 4, loss is 0.5459182858467102\n",
      "epoch: 5 step: 5, loss is 0.5555720925331116\n",
      "epoch: 5 step: 6, loss is 0.5792428851127625\n",
      "epoch: 5 step: 7, loss is 0.5727918148040771\n",
      "epoch: 5 step: 8, loss is 0.5760179758071899\n",
      "epoch: 5 step: 9, loss is 0.5824596285820007\n",
      "epoch: 5 step: 10, loss is 0.544830322265625\n",
      "epoch: 5 step: 11, loss is 0.5458979606628418\n",
      "epoch: 5 step: 12, loss is 0.5545040369033813\n",
      "epoch: 5 step: 13, loss is 0.5501737594604492\n",
      "epoch: 5 step: 14, loss is 0.5641674399375916\n",
      "epoch: 5 step: 15, loss is 0.5684508681297302\n",
      "epoch: 5 step: 16, loss is 0.5706055164337158\n",
      "epoch: 5 step: 17, loss is 0.5641647577285767\n",
      "epoch: 5 step: 18, loss is 0.5512526631355286\n",
      "epoch: 5 step: 19, loss is 0.5759977698326111\n",
      "epoch: 5 step: 20, loss is 0.5813679695129395\n",
      "epoch: 5 step: 21, loss is 0.5705617070198059\n",
      "epoch: 5 step: 22, loss is 0.5383793115615845\n",
      "epoch: 5 step: 23, loss is 0.5684487223625183\n",
      "epoch: 5 step: 24, loss is 0.5630549192428589\n",
      "epoch: 5 step: 25, loss is 0.551254391670227\n",
      "epoch: 5 step: 26, loss is 0.5748684406280518\n",
      "epoch: 5 step: 27, loss is 0.5544750690460205\n",
      "epoch: 5 step: 28, loss is 0.5609018206596375\n",
      "epoch: 5 step: 29, loss is 0.5705543756484985\n",
      "epoch: 5 step: 30, loss is 0.5726829767227173\n",
      "epoch: 5 step: 31, loss is 0.5695042610168457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 32, loss is 0.5383779406547546\n",
      "epoch: 5 step: 33, loss is 0.5566104054450989\n",
      "epoch: 5 step: 34, loss is 0.5362192988395691\n",
      "epoch: 5 step: 35, loss is 0.5544441938400269\n",
      "epoch: 5 step: 36, loss is 0.5769835114479065\n",
      "epoch: 5 step: 37, loss is 0.5737611651420593\n",
      "epoch: 5 step: 38, loss is 0.5587256550788879\n",
      "epoch: 5 step: 39, loss is 0.5705738067626953\n",
      "epoch: 5 step: 40, loss is 0.5694553852081299\n",
      "epoch: 5 step: 41, loss is 0.5748088359832764\n",
      "epoch: 5 step: 42, loss is 0.5544262528419495\n",
      "epoch: 5 step: 43, loss is 0.552304744720459\n",
      "epoch: 5 step: 44, loss is 0.5780379772186279\n",
      "epoch: 5 step: 45, loss is 0.5426087379455566\n",
      "epoch: 5 step: 46, loss is 0.5372704863548279\n",
      "epoch: 5 step: 47, loss is 0.5308413505554199\n",
      "epoch: 5 step: 48, loss is 0.5876771807670593\n",
      "epoch: 5 step: 49, loss is 0.5619386434555054\n",
      "epoch: 5 step: 50, loss is 0.5468803644180298\n",
      "epoch: 5 step: 51, loss is 0.567294180393219\n",
      "epoch: 5 step: 52, loss is 0.5715814828872681\n",
      "epoch: 5 step: 53, loss is 0.5618883371353149\n",
      "epoch: 5 step: 54, loss is 0.5619122982025146\n",
      "epoch: 5 step: 55, loss is 0.555423378944397\n",
      "epoch: 5 step: 56, loss is 0.5726818442344666\n",
      "epoch: 5 step: 57, loss is 0.5683393478393555\n",
      "epoch: 5 step: 58, loss is 0.5392919778823853\n",
      "epoch: 5 step: 59, loss is 0.5414324402809143\n",
      "epoch: 5 step: 60, loss is 0.5726869106292725\n",
      "epoch: 5 step: 61, loss is 0.5543619394302368\n",
      "epoch: 5 step: 62, loss is 0.550011932849884\n",
      "epoch: 5 step: 63, loss is 0.568362832069397\n",
      "epoch: 5 step: 64, loss is 0.5608587265014648\n",
      "epoch: 5 step: 65, loss is 0.5640518069267273\n",
      "epoch: 5 step: 66, loss is 0.5489234328269958\n",
      "epoch: 5 step: 67, loss is 0.5802190899848938\n",
      "epoch: 5 step: 68, loss is 0.567279040813446\n",
      "epoch: 5 step: 69, loss is 0.5877984166145325\n",
      "epoch: 5 step: 70, loss is 0.550011157989502\n",
      "epoch: 5 step: 71, loss is 0.5769736766815186\n",
      "epoch: 5 step: 72, loss is 0.5672311186790466\n",
      "epoch: 5 step: 73, loss is 0.5489243268966675\n",
      "epoch: 5 step: 74, loss is 0.561817467212677\n",
      "epoch: 5 step: 75, loss is 0.55106121301651\n",
      "epoch: 5 step: 76, loss is 0.5553926229476929\n",
      "epoch: 5 step: 77, loss is 0.5650768876075745\n",
      "epoch: 5 step: 78, loss is 0.5790961384773254\n",
      "epoch: 5 step: 79, loss is 0.5575017333030701\n",
      "epoch: 5 step: 80, loss is 0.5564496517181396\n",
      "epoch: 5 step: 81, loss is 0.5521200299263\n",
      "epoch: 5 step: 82, loss is 0.5521121025085449\n",
      "epoch: 5 step: 83, loss is 0.5564300417900085\n",
      "epoch: 5 step: 84, loss is 0.5467329621315002\n",
      "epoch: 5 step: 85, loss is 0.5736116170883179\n",
      "epoch: 5 step: 86, loss is 0.557497501373291\n",
      "epoch: 5 step: 87, loss is 0.5445573925971985\n",
      "epoch: 5 step: 88, loss is 0.5693363547325134\n",
      "epoch: 5 step: 89, loss is 0.5510270595550537\n",
      "epoch: 5 step: 90, loss is 0.568259596824646\n",
      "epoch: 5 step: 91, loss is 0.552024245262146\n",
      "epoch: 5 step: 92, loss is 0.563950777053833\n",
      "epoch: 5 step: 93, loss is 0.5369842052459717\n",
      "epoch: 5 step: 94, loss is 0.5584765076637268\n",
      "epoch: 5 step: 95, loss is 0.5942529439926147\n",
      "epoch: 5 step: 96, loss is 0.5671569108963013\n",
      "epoch: 5 step: 97, loss is 0.5703770518302917\n",
      "epoch: 5 step: 98, loss is 0.5887576341629028\n",
      "epoch: 5 step: 99, loss is 0.5433500409126282\n",
      "epoch: 5 step: 100, loss is 0.5692899227142334\n",
      "epoch: 5 step: 101, loss is 0.534755289554596\n",
      "epoch: 5 step: 102, loss is 0.5649581551551819\n",
      "epoch: 5 step: 103, loss is 0.5713987946510315\n",
      "epoch: 5 step: 104, loss is 0.5767967104911804\n",
      "epoch: 5 step: 105, loss is 0.5595695972442627\n",
      "epoch: 5 step: 106, loss is 0.5348280668258667\n",
      "epoch: 5 step: 107, loss is 0.5692500472068787\n",
      "epoch: 5 step: 108, loss is 0.5778544545173645\n",
      "epoch: 5 step: 109, loss is 0.5541476607322693\n",
      "epoch: 5 step: 110, loss is 0.5552076697349548\n",
      "epoch: 5 step: 111, loss is 0.5520029067993164\n",
      "epoch: 5 step: 112, loss is 0.572410523891449\n",
      "epoch: 5 step: 113, loss is 0.541212260723114\n",
      "epoch: 5 step: 114, loss is 0.5638127326965332\n",
      "epoch: 5 step: 115, loss is 0.5659489631652832\n",
      "epoch: 5 step: 116, loss is 0.5422545671463013\n",
      "epoch: 5 step: 117, loss is 0.5530070662498474\n",
      "epoch: 5 step: 118, loss is 0.5691624879837036\n",
      "epoch: 5 step: 119, loss is 0.578885555267334\n",
      "epoch: 5 step: 120, loss is 0.5443658232688904\n",
      "epoch: 5 step: 121, loss is 0.5389786958694458\n",
      "epoch: 5 step: 122, loss is 0.5605531930923462\n",
      "epoch: 5 step: 123, loss is 0.5583480596542358\n",
      "epoch: 5 step: 124, loss is 0.5443580746650696\n",
      "epoch: 5 step: 125, loss is 0.5539980530738831\n",
      "epoch: 5 step: 126, loss is 0.5561625361442566\n",
      "epoch: 5 step: 127, loss is 0.5735070705413818\n",
      "epoch: 5 step: 128, loss is 0.5961546897888184\n",
      "epoch: 5 step: 129, loss is 0.578905463218689\n",
      "epoch: 5 step: 130, loss is 0.5712817311286926\n",
      "epoch: 5 step: 131, loss is 0.5593875646591187\n",
      "epoch: 5 step: 132, loss is 0.5799268484115601\n",
      "epoch: 5 step: 133, loss is 0.5367623567581177\n",
      "epoch: 5 step: 134, loss is 0.5594268441200256\n",
      "epoch: 5 step: 135, loss is 0.5776713490486145\n",
      "epoch: 5 step: 136, loss is 0.5474845170974731\n",
      "epoch: 5 step: 137, loss is 0.557170569896698\n",
      "epoch: 5 step: 138, loss is 0.5722112059593201\n",
      "epoch: 5 step: 139, loss is 0.5475196838378906\n",
      "epoch: 5 step: 140, loss is 0.5830276608467102\n",
      "epoch: 5 step: 141, loss is 0.5561321973800659\n",
      "epoch: 5 step: 142, loss is 0.5603823065757751\n",
      "epoch: 5 step: 143, loss is 0.5550298094749451\n",
      "epoch: 5 step: 144, loss is 0.5571802854537964\n",
      "epoch: 5 step: 145, loss is 0.5785495638847351\n",
      "epoch: 5 step: 146, loss is 0.5528407692909241\n",
      "epoch: 5 step: 147, loss is 0.5624765753746033\n",
      "epoch: 5 step: 148, loss is 0.5807787775993347\n",
      "epoch: 5 step: 149, loss is 0.5549964308738708\n",
      "epoch: 5 step: 150, loss is 0.5463383197784424\n",
      "epoch: 5 step: 151, loss is 0.5527428388595581\n",
      "epoch: 5 step: 152, loss is 0.5517486929893494\n",
      "epoch: 5 step: 153, loss is 0.56355881690979\n",
      "epoch: 5 step: 154, loss is 0.5484865307807922\n",
      "epoch: 5 step: 155, loss is 0.547424852848053\n",
      "epoch: 5 step: 156, loss is 0.5559116005897522\n",
      "epoch: 5 step: 157, loss is 0.5634220242500305\n",
      "epoch: 5 step: 158, loss is 0.5806607604026794\n",
      "epoch: 5 step: 159, loss is 0.5526931881904602\n",
      "epoch: 5 step: 160, loss is 0.5570017099380493\n",
      "epoch: 5 step: 161, loss is 0.5603100061416626\n",
      "epoch: 5 step: 162, loss is 0.553753137588501\n",
      "epoch: 5 step: 163, loss is 0.5676934719085693\n",
      "epoch: 5 step: 164, loss is 0.5623265504837036\n",
      "epoch: 5 step: 165, loss is 0.5494198203086853\n",
      "epoch: 5 step: 166, loss is 0.5935101509094238\n",
      "epoch: 5 step: 167, loss is 0.5763410329818726\n",
      "epoch: 5 step: 168, loss is 0.5504551529884338\n",
      "epoch: 5 step: 169, loss is 0.5675973892211914\n",
      "epoch: 5 step: 170, loss is 0.5472044348716736\n",
      "epoch: 6 step: 1, loss is 0.5676204562187195\n",
      "epoch: 6 step: 2, loss is 0.5665209889411926\n",
      "epoch: 6 step: 3, loss is 0.5794591307640076\n",
      "epoch: 6 step: 4, loss is 0.5772362351417542\n",
      "epoch: 6 step: 5, loss is 0.5558003783226013\n",
      "epoch: 6 step: 6, loss is 0.5429055094718933\n",
      "epoch: 6 step: 7, loss is 0.5493784546852112\n",
      "epoch: 6 step: 8, loss is 0.5557389259338379\n",
      "epoch: 6 step: 9, loss is 0.534186840057373\n",
      "epoch: 6 step: 10, loss is 0.5728752017021179\n",
      "epoch: 6 step: 11, loss is 0.570591390132904\n",
      "epoch: 6 step: 12, loss is 0.583544909954071\n",
      "epoch: 6 step: 13, loss is 0.5470389127731323\n",
      "epoch: 6 step: 14, loss is 0.5781286954879761\n",
      "epoch: 6 step: 15, loss is 0.557719886302948\n",
      "epoch: 6 step: 16, loss is 0.5406023859977722\n",
      "epoch: 6 step: 17, loss is 0.5426830649375916\n",
      "epoch: 6 step: 18, loss is 0.5544777512550354\n",
      "epoch: 6 step: 19, loss is 0.5502071380615234\n",
      "epoch: 6 step: 20, loss is 0.5437095165252686\n",
      "epoch: 6 step: 21, loss is 0.5394035577774048\n",
      "epoch: 6 step: 22, loss is 0.5425735116004944\n",
      "epoch: 6 step: 23, loss is 0.5833728909492493\n",
      "epoch: 6 step: 24, loss is 0.5381706357002258\n",
      "epoch: 6 step: 25, loss is 0.5726427435874939\n",
      "epoch: 6 step: 26, loss is 0.5726244449615479\n",
      "epoch: 6 step: 27, loss is 0.540281355381012\n",
      "epoch: 6 step: 28, loss is 0.5791751146316528\n",
      "epoch: 6 step: 29, loss is 0.5553680062294006\n",
      "epoch: 6 step: 30, loss is 0.5737514495849609\n",
      "epoch: 6 step: 31, loss is 0.5617216229438782\n",
      "epoch: 6 step: 32, loss is 0.5465176701545715\n",
      "epoch: 6 step: 33, loss is 0.5898696780204773\n",
      "epoch: 6 step: 34, loss is 0.5769563317298889\n",
      "epoch: 6 step: 35, loss is 0.5843579173088074\n",
      "epoch: 6 step: 36, loss is 0.557293713092804\n",
      "epoch: 6 step: 37, loss is 0.5573483109474182\n",
      "epoch: 6 step: 38, loss is 0.5616193413734436\n",
      "epoch: 6 step: 39, loss is 0.5573564171791077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 40, loss is 0.5323300361633301\n",
      "epoch: 6 step: 41, loss is 0.5593745708465576\n",
      "epoch: 6 step: 42, loss is 0.5453094840049744\n",
      "epoch: 6 step: 43, loss is 0.5884336233139038\n",
      "epoch: 6 step: 44, loss is 0.5528537034988403\n",
      "epoch: 6 step: 45, loss is 0.5658349394798279\n",
      "epoch: 6 step: 46, loss is 0.5646947622299194\n",
      "epoch: 6 step: 47, loss is 0.5775450468063354\n",
      "epoch: 6 step: 48, loss is 0.5463544130325317\n",
      "epoch: 6 step: 49, loss is 0.5591160655021667\n",
      "epoch: 6 step: 50, loss is 0.5537575483322144\n",
      "epoch: 6 step: 51, loss is 0.5558692216873169\n",
      "epoch: 6 step: 52, loss is 0.5483181476593018\n",
      "epoch: 6 step: 53, loss is 0.5482674837112427\n",
      "epoch: 6 step: 54, loss is 0.5707305073738098\n",
      "epoch: 6 step: 55, loss is 0.5460650324821472\n",
      "epoch: 6 step: 56, loss is 0.5845777988433838\n",
      "epoch: 6 step: 57, loss is 0.563330352306366\n",
      "epoch: 6 step: 58, loss is 0.5514127016067505\n",
      "epoch: 6 step: 59, loss is 0.565207302570343\n",
      "epoch: 6 step: 60, loss is 0.5619938969612122\n",
      "epoch: 6 step: 61, loss is 0.5920215845108032\n",
      "epoch: 6 step: 62, loss is 0.5609074831008911\n",
      "epoch: 6 step: 63, loss is 0.5715690851211548\n",
      "epoch: 6 step: 64, loss is 0.5500736236572266\n",
      "epoch: 6 step: 65, loss is 0.5352881550788879\n",
      "epoch: 6 step: 66, loss is 0.5392784476280212\n",
      "epoch: 6 step: 67, loss is 0.5542389750480652\n",
      "epoch: 6 step: 68, loss is 0.5670109391212463\n",
      "epoch: 6 step: 69, loss is 0.5607675909996033\n",
      "epoch: 6 step: 70, loss is 0.5433458089828491\n",
      "epoch: 6 step: 71, loss is 0.5432567596435547\n",
      "epoch: 6 step: 72, loss is 0.5775516033172607\n",
      "epoch: 6 step: 73, loss is 0.5558104515075684\n",
      "epoch: 6 step: 74, loss is 0.5484123229980469\n",
      "epoch: 6 step: 75, loss is 0.5581244230270386\n",
      "epoch: 6 step: 76, loss is 0.5784491300582886\n",
      "epoch: 6 step: 77, loss is 0.5925315618515015\n",
      "epoch: 6 step: 78, loss is 0.5247325897216797\n",
      "epoch: 6 step: 79, loss is 0.5600822567939758\n",
      "epoch: 6 step: 80, loss is 0.5716118216514587\n",
      "epoch: 6 step: 81, loss is 0.5403886437416077\n",
      "epoch: 6 step: 82, loss is 0.5471629500389099\n",
      "epoch: 6 step: 83, loss is 0.566106915473938\n",
      "epoch: 6 step: 84, loss is 0.5381509065628052\n",
      "epoch: 6 step: 85, loss is 0.5859625339508057\n",
      "epoch: 6 step: 86, loss is 0.5488229990005493\n",
      "epoch: 6 step: 87, loss is 0.5561323761940002\n",
      "epoch: 6 step: 88, loss is 0.5690955519676208\n",
      "epoch: 6 step: 89, loss is 0.5505241751670837\n",
      "epoch: 6 step: 90, loss is 0.5485535860061646\n",
      "epoch: 6 step: 91, loss is 0.5474169850349426\n",
      "epoch: 6 step: 92, loss is 0.5406079292297363\n",
      "epoch: 6 step: 93, loss is 0.589107096195221\n",
      "epoch: 6 step: 94, loss is 0.5814056992530823\n",
      "epoch: 6 step: 95, loss is 0.5662301182746887\n",
      "epoch: 6 step: 96, loss is 0.5330947637557983\n",
      "epoch: 6 step: 97, loss is 0.5232027769088745\n",
      "epoch: 6 step: 98, loss is 0.5584108233451843\n",
      "epoch: 6 step: 99, loss is 0.5571470856666565\n",
      "epoch: 6 step: 100, loss is 0.5889780521392822\n",
      "epoch: 6 step: 101, loss is 0.5491899251937866\n",
      "epoch: 6 step: 102, loss is 0.5636876821517944\n",
      "epoch: 6 step: 103, loss is 0.5732486844062805\n",
      "epoch: 6 step: 104, loss is 0.5502826571464539\n",
      "epoch: 6 step: 105, loss is 0.5352777242660522\n",
      "epoch: 6 step: 106, loss is 0.5512494444847107\n",
      "epoch: 6 step: 107, loss is 0.5679633021354675\n",
      "epoch: 6 step: 108, loss is 0.5519286394119263\n",
      "epoch: 6 step: 109, loss is 0.5615956783294678\n",
      "epoch: 6 step: 110, loss is 0.5577389597892761\n",
      "epoch: 6 step: 111, loss is 0.5782608389854431\n",
      "epoch: 6 step: 112, loss is 0.5571137070655823\n",
      "epoch: 6 step: 113, loss is 0.5281944870948792\n",
      "epoch: 6 step: 114, loss is 0.5245594382286072\n",
      "epoch: 6 step: 115, loss is 0.5654488801956177\n",
      "epoch: 6 step: 116, loss is 0.5608038902282715\n",
      "epoch: 6 step: 117, loss is 0.5473600625991821\n",
      "epoch: 6 step: 118, loss is 0.5554285645484924\n",
      "epoch: 6 step: 119, loss is 0.5584762096405029\n",
      "epoch: 6 step: 120, loss is 0.5470539331436157\n",
      "epoch: 6 step: 121, loss is 0.5472756028175354\n",
      "epoch: 6 step: 122, loss is 0.5609345436096191\n",
      "epoch: 6 step: 123, loss is 0.5714832544326782\n",
      "epoch: 6 step: 124, loss is 0.5430676937103271\n",
      "epoch: 6 step: 125, loss is 0.55829256772995\n",
      "epoch: 6 step: 126, loss is 0.5684537887573242\n",
      "epoch: 6 step: 127, loss is 0.5404245257377625\n",
      "epoch: 6 step: 128, loss is 0.5516409873962402\n",
      "epoch: 6 step: 129, loss is 0.5112972855567932\n",
      "epoch: 6 step: 130, loss is 0.5425736308097839\n",
      "epoch: 6 step: 131, loss is 0.537284791469574\n",
      "epoch: 6 step: 132, loss is 0.5612717866897583\n",
      "epoch: 6 step: 133, loss is 0.551262378692627\n",
      "epoch: 6 step: 134, loss is 0.53361976146698\n",
      "epoch: 6 step: 135, loss is 0.5639986395835876\n",
      "epoch: 6 step: 136, loss is 0.564731776714325\n",
      "epoch: 6 step: 137, loss is 0.5635865926742554\n",
      "epoch: 6 step: 138, loss is 0.540477454662323\n",
      "epoch: 6 step: 139, loss is 0.5602285861968994\n",
      "epoch: 6 step: 140, loss is 0.5379601120948792\n",
      "epoch: 6 step: 141, loss is 0.539757251739502\n",
      "epoch: 6 step: 142, loss is 0.5512436032295227\n",
      "epoch: 6 step: 143, loss is 0.5219876766204834\n",
      "epoch: 6 step: 144, loss is 0.5400108098983765\n",
      "epoch: 6 step: 145, loss is 0.5098075866699219\n",
      "epoch: 6 step: 146, loss is 0.5291693210601807\n",
      "epoch: 6 step: 147, loss is 0.5230626463890076\n",
      "epoch: 6 step: 148, loss is 0.5209330320358276\n",
      "epoch: 6 step: 149, loss is 0.5460148453712463\n",
      "epoch: 6 step: 150, loss is 0.5423649549484253\n",
      "epoch: 6 step: 151, loss is 0.5683444142341614\n",
      "epoch: 6 step: 152, loss is 0.5325637459754944\n",
      "epoch: 6 step: 153, loss is 0.5691161751747131\n",
      "epoch: 6 step: 154, loss is 0.5490321516990662\n",
      "epoch: 6 step: 155, loss is 0.5167955160140991\n",
      "epoch: 6 step: 156, loss is 0.507034182548523\n",
      "epoch: 6 step: 157, loss is 0.5548051595687866\n",
      "epoch: 6 step: 158, loss is 0.49507737159729004\n",
      "epoch: 6 step: 159, loss is 0.5331563353538513\n",
      "epoch: 6 step: 160, loss is 0.567829430103302\n",
      "epoch: 6 step: 161, loss is 0.5092540383338928\n",
      "epoch: 6 step: 162, loss is 0.5252929329872131\n",
      "epoch: 6 step: 163, loss is 0.521698534488678\n",
      "epoch: 6 step: 164, loss is 0.5220616459846497\n",
      "epoch: 6 step: 165, loss is 0.4687584936618805\n",
      "epoch: 6 step: 166, loss is 0.4759078323841095\n",
      "epoch: 6 step: 167, loss is 0.5060440897941589\n",
      "epoch: 6 step: 168, loss is 0.504835844039917\n",
      "epoch: 6 step: 169, loss is 0.5115960836410522\n",
      "epoch: 6 step: 170, loss is 0.48432663083076477\n",
      "epoch: 7 step: 1, loss is 0.510260283946991\n",
      "epoch: 7 step: 2, loss is 0.4817831516265869\n",
      "epoch: 7 step: 3, loss is 0.45994386076927185\n",
      "epoch: 7 step: 4, loss is 0.4620351791381836\n",
      "epoch: 7 step: 5, loss is 0.49005216360092163\n",
      "epoch: 7 step: 6, loss is 0.4591354429721832\n",
      "epoch: 7 step: 7, loss is 0.4897867739200592\n",
      "epoch: 7 step: 8, loss is 0.47508466243743896\n",
      "epoch: 7 step: 9, loss is 0.45007115602493286\n",
      "epoch: 7 step: 10, loss is 0.44315555691719055\n",
      "epoch: 7 step: 11, loss is 0.45587727427482605\n",
      "epoch: 7 step: 12, loss is 0.43274474143981934\n",
      "epoch: 7 step: 13, loss is 0.4476960301399231\n",
      "epoch: 7 step: 14, loss is 0.4248761534690857\n",
      "epoch: 7 step: 15, loss is 0.4646854102611542\n",
      "epoch: 7 step: 16, loss is 0.43594369292259216\n",
      "epoch: 7 step: 17, loss is 0.41787636280059814\n",
      "epoch: 7 step: 18, loss is 0.41988620162010193\n",
      "epoch: 7 step: 19, loss is 0.4199436902999878\n",
      "epoch: 7 step: 20, loss is 0.38403528928756714\n",
      "epoch: 7 step: 21, loss is 0.4521665573120117\n",
      "epoch: 7 step: 22, loss is 0.46433696150779724\n",
      "epoch: 7 step: 23, loss is 0.44844451546669006\n",
      "epoch: 7 step: 24, loss is 0.5554100871086121\n",
      "epoch: 7 step: 25, loss is 0.4098828136920929\n",
      "epoch: 7 step: 26, loss is 0.4402969181537628\n",
      "epoch: 7 step: 27, loss is 0.45523372292518616\n",
      "epoch: 7 step: 28, loss is 0.402752548456192\n",
      "epoch: 7 step: 29, loss is 0.41639161109924316\n",
      "epoch: 7 step: 30, loss is 0.4544696509838104\n",
      "epoch: 7 step: 31, loss is 0.44197338819503784\n",
      "epoch: 7 step: 32, loss is 0.4465723931789398\n",
      "epoch: 7 step: 33, loss is 0.39712637662887573\n",
      "epoch: 7 step: 34, loss is 0.562737762928009\n",
      "epoch: 7 step: 35, loss is 0.5248847007751465\n",
      "epoch: 7 step: 36, loss is 0.472533255815506\n",
      "epoch: 7 step: 37, loss is 0.5847111344337463\n",
      "epoch: 7 step: 38, loss is 0.4907970428466797\n",
      "epoch: 7 step: 39, loss is 0.621336042881012\n",
      "epoch: 7 step: 40, loss is 0.6128140687942505\n",
      "epoch: 7 step: 41, loss is 0.6084981560707092\n",
      "epoch: 7 step: 42, loss is 0.6225807070732117\n",
      "epoch: 7 step: 43, loss is 0.6199144124984741\n",
      "epoch: 7 step: 44, loss is 0.6186084747314453\n",
      "epoch: 7 step: 45, loss is 0.6171994805335999\n",
      "epoch: 7 step: 46, loss is 0.6053689122200012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 47, loss is 0.6164376735687256\n",
      "epoch: 7 step: 48, loss is 0.6000385880470276\n",
      "epoch: 7 step: 49, loss is 0.6022570133209229\n",
      "epoch: 7 step: 50, loss is 0.6050218939781189\n",
      "epoch: 7 step: 51, loss is 0.5940548777580261\n",
      "epoch: 7 step: 52, loss is 0.6054550409317017\n",
      "epoch: 7 step: 53, loss is 0.5991603136062622\n",
      "epoch: 7 step: 54, loss is 0.5898312926292419\n",
      "epoch: 7 step: 55, loss is 0.5889089703559875\n",
      "epoch: 7 step: 56, loss is 0.580732524394989\n",
      "epoch: 7 step: 57, loss is 0.5918401479721069\n",
      "epoch: 7 step: 58, loss is 0.5793980956077576\n",
      "epoch: 7 step: 59, loss is 0.5669751763343811\n",
      "epoch: 7 step: 60, loss is 0.5849604606628418\n",
      "epoch: 7 step: 61, loss is 0.5819002389907837\n",
      "epoch: 7 step: 62, loss is 0.5761282444000244\n",
      "epoch: 7 step: 63, loss is 0.5616493821144104\n",
      "epoch: 7 step: 64, loss is 0.5812408328056335\n",
      "epoch: 7 step: 65, loss is 0.5583826303482056\n",
      "epoch: 7 step: 66, loss is 0.5705623030662537\n",
      "epoch: 7 step: 67, loss is 0.589332103729248\n",
      "epoch: 7 step: 68, loss is 0.5744246244430542\n",
      "epoch: 7 step: 69, loss is 0.5725796222686768\n",
      "epoch: 7 step: 70, loss is 0.5575430989265442\n",
      "epoch: 7 step: 71, loss is 0.579865574836731\n",
      "epoch: 7 step: 72, loss is 0.5585151314735413\n",
      "epoch: 7 step: 73, loss is 0.5454258322715759\n",
      "epoch: 7 step: 74, loss is 0.5755087733268738\n",
      "epoch: 7 step: 75, loss is 0.5575881004333496\n",
      "epoch: 7 step: 76, loss is 0.5617477893829346\n",
      "epoch: 7 step: 77, loss is 0.5623101592063904\n",
      "epoch: 7 step: 78, loss is 0.5610529184341431\n",
      "epoch: 7 step: 79, loss is 0.560762882232666\n",
      "epoch: 7 step: 80, loss is 0.5760104656219482\n",
      "epoch: 7 step: 81, loss is 0.5456941723823547\n",
      "epoch: 7 step: 82, loss is 0.558147668838501\n",
      "epoch: 7 step: 83, loss is 0.5881606936454773\n",
      "epoch: 7 step: 84, loss is 0.5828790664672852\n",
      "epoch: 7 step: 85, loss is 0.5486095547676086\n",
      "epoch: 7 step: 86, loss is 0.5525022149085999\n",
      "epoch: 7 step: 87, loss is 0.5728395581245422\n",
      "epoch: 7 step: 88, loss is 0.5443170070648193\n",
      "epoch: 7 step: 89, loss is 0.5462389588356018\n",
      "epoch: 7 step: 90, loss is 0.5531043410301208\n",
      "epoch: 7 step: 91, loss is 0.5843368172645569\n",
      "epoch: 7 step: 92, loss is 0.5404224991798401\n",
      "epoch: 7 step: 93, loss is 0.536061704158783\n",
      "epoch: 7 step: 94, loss is 0.5729305744171143\n",
      "epoch: 7 step: 95, loss is 0.5538567900657654\n",
      "epoch: 7 step: 96, loss is 0.5623420476913452\n",
      "epoch: 7 step: 97, loss is 0.5698201656341553\n",
      "epoch: 7 step: 98, loss is 0.580600380897522\n",
      "epoch: 7 step: 99, loss is 0.5558933615684509\n",
      "epoch: 7 step: 100, loss is 0.573093831539154\n",
      "epoch: 7 step: 101, loss is 0.5709590911865234\n",
      "epoch: 7 step: 102, loss is 0.5774468183517456\n",
      "epoch: 7 step: 103, loss is 0.5536942481994629\n",
      "epoch: 7 step: 104, loss is 0.5461256504058838\n",
      "epoch: 7 step: 105, loss is 0.5558515787124634\n",
      "epoch: 7 step: 106, loss is 0.5612592697143555\n",
      "epoch: 7 step: 107, loss is 0.5547590851783752\n",
      "epoch: 7 step: 108, loss is 0.5659427642822266\n",
      "epoch: 7 step: 109, loss is 0.5645251870155334\n",
      "epoch: 7 step: 110, loss is 0.5713363885879517\n",
      "epoch: 7 step: 111, loss is 0.5819305777549744\n",
      "epoch: 7 step: 112, loss is 0.5699961185455322\n",
      "epoch: 7 step: 113, loss is 0.5406151413917542\n",
      "epoch: 7 step: 114, loss is 0.5841038227081299\n",
      "epoch: 7 step: 115, loss is 0.5319188833236694\n",
      "epoch: 7 step: 116, loss is 0.5678267478942871\n",
      "epoch: 7 step: 117, loss is 0.5503969192504883\n",
      "epoch: 7 step: 118, loss is 0.5721435546875\n",
      "epoch: 7 step: 119, loss is 0.5753967761993408\n",
      "epoch: 7 step: 120, loss is 0.5702862739562988\n",
      "epoch: 7 step: 121, loss is 0.5156478881835938\n",
      "epoch: 7 step: 122, loss is 0.5666984915733337\n",
      "epoch: 7 step: 123, loss is 0.5460565090179443\n",
      "epoch: 7 step: 124, loss is 0.5645312666893005\n",
      "epoch: 7 step: 125, loss is 0.5580098628997803\n",
      "epoch: 7 step: 126, loss is 0.5514862537384033\n",
      "epoch: 7 step: 127, loss is 0.5656262040138245\n",
      "epoch: 7 step: 128, loss is 0.5710893869400024\n",
      "epoch: 7 step: 129, loss is 0.5626373291015625\n",
      "epoch: 7 step: 130, loss is 0.5917328000068665\n",
      "epoch: 7 step: 131, loss is 0.5797889232635498\n",
      "epoch: 7 step: 132, loss is 0.5775588750839233\n",
      "epoch: 7 step: 133, loss is 0.5504112839698792\n",
      "epoch: 7 step: 134, loss is 0.5374143123626709\n",
      "epoch: 7 step: 135, loss is 0.5558421611785889\n",
      "epoch: 7 step: 136, loss is 0.5883521437644958\n",
      "epoch: 7 step: 137, loss is 0.536359965801239\n",
      "epoch: 7 step: 138, loss is 0.5691162943840027\n",
      "epoch: 7 step: 139, loss is 0.5742483139038086\n",
      "epoch: 7 step: 140, loss is 0.5582866072654724\n",
      "epoch: 7 step: 141, loss is 0.5461305975914001\n",
      "epoch: 7 step: 142, loss is 0.5958448052406311\n",
      "epoch: 7 step: 143, loss is 0.5688216686248779\n",
      "epoch: 7 step: 144, loss is 0.542913019657135\n",
      "epoch: 7 step: 145, loss is 0.5591045618057251\n",
      "epoch: 7 step: 146, loss is 0.5677329301834106\n",
      "epoch: 7 step: 147, loss is 0.5558691620826721\n",
      "epoch: 7 step: 148, loss is 0.5634607076644897\n",
      "epoch: 7 step: 149, loss is 0.5515615344047546\n",
      "epoch: 7 step: 150, loss is 0.5849548578262329\n",
      "epoch: 7 step: 151, loss is 0.5623325705528259\n",
      "epoch: 7 step: 152, loss is 0.5473039746284485\n",
      "epoch: 7 step: 153, loss is 0.5623577833175659\n",
      "epoch: 7 step: 154, loss is 0.5504977703094482\n",
      "epoch: 7 step: 155, loss is 0.5408210158348083\n",
      "epoch: 7 step: 156, loss is 0.5709414482116699\n",
      "epoch: 7 step: 157, loss is 0.5827928781509399\n",
      "epoch: 7 step: 158, loss is 0.5601805448532104\n",
      "epoch: 7 step: 159, loss is 0.5623326301574707\n",
      "epoch: 7 step: 160, loss is 0.5461964011192322\n",
      "epoch: 7 step: 161, loss is 0.5731432437896729\n",
      "epoch: 7 step: 162, loss is 0.5548012256622314\n",
      "epoch: 7 step: 163, loss is 0.5644944906234741\n",
      "epoch: 7 step: 164, loss is 0.5494499206542969\n",
      "epoch: 7 step: 165, loss is 0.5763342976570129\n",
      "epoch: 7 step: 166, loss is 0.5569517016410828\n",
      "epoch: 7 step: 167, loss is 0.5742092728614807\n",
      "epoch: 7 step: 168, loss is 0.5618119239807129\n",
      "epoch: 7 step: 169, loss is 0.5418941974639893\n",
      "epoch: 7 step: 170, loss is 0.5806348323822021\n",
      "epoch: 8 step: 1, loss is 0.5354374647140503\n",
      "epoch: 8 step: 2, loss is 0.5709448456764221\n",
      "epoch: 8 step: 3, loss is 0.574198305606842\n",
      "epoch: 8 step: 4, loss is 0.5558759570121765\n",
      "epoch: 8 step: 5, loss is 0.5496876239776611\n",
      "epoch: 8 step: 6, loss is 0.545107901096344\n",
      "epoch: 8 step: 7, loss is 0.5504903793334961\n",
      "epoch: 8 step: 8, loss is 0.5440174341201782\n",
      "epoch: 8 step: 9, loss is 0.5569444298744202\n",
      "epoch: 8 step: 10, loss is 0.5591006875038147\n",
      "epoch: 8 step: 11, loss is 0.5526200532913208\n",
      "epoch: 8 step: 12, loss is 0.5612635612487793\n",
      "epoch: 8 step: 13, loss is 0.5645062327384949\n",
      "epoch: 8 step: 14, loss is 0.5756491422653198\n",
      "epoch: 8 step: 15, loss is 0.5840038061141968\n",
      "epoch: 8 step: 16, loss is 0.603489100933075\n",
      "epoch: 8 step: 17, loss is 0.560224711894989\n",
      "epoch: 8 step: 18, loss is 0.5723403692245483\n",
      "epoch: 8 step: 19, loss is 0.5537016987800598\n",
      "epoch: 8 step: 20, loss is 0.54506915807724\n",
      "epoch: 8 step: 21, loss is 0.5526235699653625\n",
      "epoch: 8 step: 22, loss is 0.5493974685668945\n",
      "epoch: 8 step: 23, loss is 0.5677334666252136\n",
      "epoch: 8 step: 24, loss is 0.5569472312927246\n",
      "epoch: 8 step: 25, loss is 0.5701672434806824\n",
      "epoch: 8 step: 26, loss is 0.5720961093902588\n",
      "epoch: 8 step: 27, loss is 0.5634179711341858\n",
      "epoch: 8 step: 28, loss is 0.6151533722877502\n",
      "epoch: 8 step: 29, loss is 0.5451398491859436\n",
      "epoch: 8 step: 30, loss is 0.5537334084510803\n",
      "epoch: 8 step: 31, loss is 0.5752390623092651\n",
      "epoch: 8 step: 32, loss is 0.5440755486488342\n",
      "epoch: 8 step: 33, loss is 0.5472989678382874\n",
      "epoch: 8 step: 34, loss is 0.5580395460128784\n",
      "epoch: 8 step: 35, loss is 0.5422006249427795\n",
      "epoch: 8 step: 36, loss is 0.5623390078544617\n",
      "epoch: 8 step: 37, loss is 0.5849031805992126\n",
      "epoch: 8 step: 38, loss is 0.5558871030807495\n",
      "epoch: 8 step: 39, loss is 0.5526638627052307\n",
      "epoch: 8 step: 40, loss is 0.5763047337532043\n",
      "epoch: 8 step: 41, loss is 0.5816795825958252\n",
      "epoch: 8 step: 42, loss is 0.5558972358703613\n",
      "epoch: 8 step: 43, loss is 0.552702784538269\n",
      "epoch: 8 step: 44, loss is 0.5397919416427612\n",
      "epoch: 8 step: 45, loss is 0.5752198696136475\n",
      "epoch: 8 step: 46, loss is 0.5322690010070801\n",
      "epoch: 8 step: 47, loss is 0.5669712424278259\n",
      "epoch: 8 step: 48, loss is 0.5601862072944641\n",
      "epoch: 8 step: 49, loss is 0.5548141598701477\n",
      "epoch: 8 step: 50, loss is 0.5860061049461365\n",
      "epoch: 8 step: 51, loss is 0.5655649304389954\n",
      "epoch: 8 step: 52, loss is 0.5773963332176208\n",
      "epoch: 8 step: 53, loss is 0.5505033731460571\n",
      "epoch: 8 step: 54, loss is 0.5752750635147095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 55, loss is 0.5537384748458862\n",
      "epoch: 8 step: 56, loss is 0.5483616590499878\n",
      "epoch: 8 step: 57, loss is 0.5634079575538635\n",
      "epoch: 8 step: 58, loss is 0.5741593837738037\n",
      "epoch: 8 step: 59, loss is 0.5559030771255493\n",
      "epoch: 8 step: 60, loss is 0.5518501996994019\n",
      "epoch: 8 step: 61, loss is 0.5204090476036072\n",
      "epoch: 8 step: 62, loss is 0.5730948448181152\n",
      "epoch: 8 step: 63, loss is 0.578804612159729\n",
      "epoch: 8 step: 64, loss is 0.5698712468147278\n",
      "epoch: 8 step: 65, loss is 0.551567554473877\n",
      "epoch: 8 step: 66, loss is 0.578498125076294\n",
      "epoch: 8 step: 67, loss is 0.5612616539001465\n",
      "epoch: 8 step: 68, loss is 0.5655689835548401\n",
      "epoch: 8 step: 69, loss is 0.5580307841300964\n",
      "epoch: 8 step: 70, loss is 0.5720290541648865\n",
      "epoch: 8 step: 71, loss is 0.5593948364257812\n",
      "epoch: 8 step: 72, loss is 0.566638708114624\n",
      "epoch: 8 step: 73, loss is 0.5440792441368103\n",
      "epoch: 8 step: 74, loss is 0.5494251847267151\n",
      "epoch: 8 step: 75, loss is 0.5321938395500183\n",
      "epoch: 8 step: 76, loss is 0.5440271496772766\n",
      "epoch: 8 step: 77, loss is 0.5741986632347107\n",
      "epoch: 8 step: 78, loss is 0.56880784034729\n",
      "epoch: 8 step: 79, loss is 0.5590996146202087\n",
      "epoch: 8 step: 80, loss is 0.5850155353546143\n",
      "epoch: 8 step: 81, loss is 0.5626001358032227\n",
      "epoch: 8 step: 82, loss is 0.5591026544570923\n",
      "epoch: 8 step: 83, loss is 0.5839813351631165\n",
      "epoch: 8 step: 84, loss is 0.5483058094978333\n",
      "epoch: 8 step: 85, loss is 0.5860816240310669\n",
      "epoch: 8 step: 86, loss is 0.5644937753677368\n",
      "epoch: 8 step: 87, loss is 0.5774307250976562\n",
      "epoch: 8 step: 88, loss is 0.5526387095451355\n",
      "epoch: 8 step: 89, loss is 0.5548038482666016\n",
      "epoch: 8 step: 90, loss is 0.5698656439781189\n",
      "epoch: 8 step: 91, loss is 0.5343673229217529\n",
      "epoch: 8 step: 92, loss is 0.572086751461029\n",
      "epoch: 8 step: 93, loss is 0.5687932968139648\n",
      "epoch: 8 step: 94, loss is 0.5615240335464478\n",
      "epoch: 8 step: 95, loss is 0.5698633193969727\n",
      "epoch: 8 step: 96, loss is 0.5798025131225586\n",
      "epoch: 8 step: 97, loss is 0.5644814968109131\n",
      "epoch: 8 step: 98, loss is 0.5634065270423889\n",
      "epoch: 8 step: 99, loss is 0.5548240542411804\n",
      "epoch: 8 step: 100, loss is 0.5419632792472839\n",
      "epoch: 8 step: 101, loss is 0.5698440074920654\n",
      "epoch: 8 step: 102, loss is 0.5505495667457581\n",
      "epoch: 8 step: 103, loss is 0.570946991443634\n",
      "epoch: 8 step: 104, loss is 0.5762816667556763\n",
      "epoch: 8 step: 105, loss is 0.5475921034812927\n",
      "epoch: 8 step: 106, loss is 0.5944997072219849\n",
      "epoch: 8 step: 107, loss is 0.5762697458267212\n",
      "epoch: 8 step: 108, loss is 0.5634029507637024\n",
      "epoch: 8 step: 109, loss is 0.5794548392295837\n",
      "epoch: 8 step: 110, loss is 0.5441588759422302\n",
      "epoch: 8 step: 111, loss is 0.5698135495185852\n",
      "epoch: 8 step: 112, loss is 0.5497771501541138\n",
      "epoch: 8 step: 113, loss is 0.5580604076385498\n",
      "epoch: 8 step: 114, loss is 0.5388514995574951\n",
      "epoch: 8 step: 115, loss is 0.5313627123832703\n",
      "epoch: 8 step: 116, loss is 0.5462978482246399\n",
      "epoch: 8 step: 117, loss is 0.5634036064147949\n",
      "epoch: 8 step: 118, loss is 0.5741187930107117\n",
      "epoch: 8 step: 119, loss is 0.5613148212432861\n",
      "epoch: 8 step: 120, loss is 0.5795369148254395\n",
      "epoch: 8 step: 121, loss is 0.5623361468315125\n",
      "epoch: 8 step: 122, loss is 0.5744063258171082\n",
      "epoch: 8 step: 123, loss is 0.5794973373413086\n",
      "epoch: 8 step: 124, loss is 0.5655556321144104\n",
      "epoch: 8 step: 125, loss is 0.5497305989265442\n",
      "epoch: 8 step: 126, loss is 0.5419660210609436\n",
      "epoch: 8 step: 127, loss is 0.5537585020065308\n",
      "epoch: 8 step: 128, loss is 0.5430252552032471\n",
      "epoch: 8 step: 129, loss is 0.5709229707717896\n",
      "epoch: 8 step: 130, loss is 0.5526644587516785\n",
      "epoch: 8 step: 131, loss is 0.5677090883255005\n",
      "epoch: 8 step: 132, loss is 0.5569600462913513\n",
      "epoch: 8 step: 133, loss is 0.5580342411994934\n",
      "epoch: 8 step: 134, loss is 0.5623352527618408\n",
      "epoch: 8 step: 135, loss is 0.5688234567642212\n",
      "epoch: 8 step: 136, loss is 0.5612742900848389\n",
      "epoch: 8 step: 137, loss is 0.5547915101051331\n",
      "epoch: 8 step: 138, loss is 0.5561355352401733\n",
      "epoch: 8 step: 139, loss is 0.5741949081420898\n",
      "epoch: 8 step: 140, loss is 0.548321545124054\n",
      "epoch: 8 step: 141, loss is 0.5634887218475342\n",
      "epoch: 8 step: 142, loss is 0.5634215474128723\n",
      "epoch: 8 step: 143, loss is 0.5591158866882324\n",
      "epoch: 8 step: 144, loss is 0.5785283446311951\n",
      "epoch: 8 step: 145, loss is 0.548566460609436\n",
      "epoch: 8 step: 146, loss is 0.554787278175354\n",
      "epoch: 8 step: 147, loss is 0.5698947310447693\n",
      "epoch: 8 step: 148, loss is 0.5720590353012085\n",
      "epoch: 8 step: 149, loss is 0.5407556891441345\n",
      "epoch: 8 step: 150, loss is 0.5655770301818848\n",
      "epoch: 8 step: 151, loss is 0.5526197552680969\n",
      "epoch: 8 step: 152, loss is 0.5601752400398254\n",
      "epoch: 8 step: 153, loss is 0.5785366892814636\n",
      "epoch: 8 step: 154, loss is 0.5353406667709351\n",
      "epoch: 8 step: 155, loss is 0.5472158789634705\n",
      "epoch: 8 step: 156, loss is 0.5839605927467346\n",
      "epoch: 8 step: 157, loss is 0.5777269005775452\n",
      "epoch: 8 step: 158, loss is 0.5385596752166748\n",
      "epoch: 8 step: 159, loss is 0.5666666030883789\n",
      "epoch: 8 step: 160, loss is 0.581826388835907\n",
      "epoch: 8 step: 161, loss is 0.5374851822853088\n",
      "epoch: 8 step: 162, loss is 0.5472115874290466\n",
      "epoch: 8 step: 163, loss is 0.5699126124382019\n",
      "epoch: 8 step: 164, loss is 0.5904588103294373\n",
      "epoch: 8 step: 165, loss is 0.5431522727012634\n",
      "epoch: 8 step: 166, loss is 0.5677445530891418\n",
      "epoch: 8 step: 167, loss is 0.5494066476821899\n",
      "epoch: 8 step: 168, loss is 0.5504563450813293\n",
      "epoch: 8 step: 169, loss is 0.575325608253479\n",
      "epoch: 8 step: 170, loss is 0.5861198902130127\n",
      "epoch: 9 step: 1, loss is 0.5601802468299866\n",
      "epoch: 9 step: 2, loss is 0.5698961615562439\n",
      "epoch: 9 step: 3, loss is 0.5601792931556702\n",
      "epoch: 9 step: 4, loss is 0.5688117146492004\n",
      "epoch: 9 step: 5, loss is 0.563414454460144\n",
      "epoch: 9 step: 6, loss is 0.5601822733879089\n",
      "epoch: 9 step: 7, loss is 0.5677224397659302\n",
      "epoch: 9 step: 8, loss is 0.5582982301712036\n",
      "epoch: 9 step: 9, loss is 0.5644944906234741\n",
      "epoch: 9 step: 10, loss is 0.5300555229187012\n",
      "epoch: 9 step: 11, loss is 0.5591092109680176\n",
      "epoch: 9 step: 12, loss is 0.5419102907180786\n",
      "epoch: 9 step: 13, loss is 0.5539789795875549\n",
      "epoch: 9 step: 14, loss is 0.553719162940979\n",
      "epoch: 9 step: 15, loss is 0.5893300175666809\n",
      "epoch: 9 step: 16, loss is 0.5655860304832458\n",
      "epoch: 9 step: 17, loss is 0.5763520002365112\n",
      "epoch: 9 step: 18, loss is 0.5828261971473694\n",
      "epoch: 9 step: 19, loss is 0.5634164810180664\n",
      "epoch: 9 step: 20, loss is 0.5429592132568359\n",
      "epoch: 9 step: 21, loss is 0.5461871027946472\n",
      "epoch: 9 step: 22, loss is 0.5763293504714966\n",
      "epoch: 9 step: 23, loss is 0.5841212272644043\n",
      "epoch: 9 step: 24, loss is 0.5526543259620667\n",
      "epoch: 9 step: 25, loss is 0.5644837021827698\n",
      "epoch: 9 step: 26, loss is 0.5558786988258362\n",
      "epoch: 9 step: 27, loss is 0.5279321670532227\n",
      "epoch: 9 step: 28, loss is 0.5558815598487854\n",
      "epoch: 9 step: 29, loss is 0.5483512282371521\n",
      "epoch: 9 step: 30, loss is 0.5741872191429138\n",
      "epoch: 9 step: 31, loss is 0.5442957282066345\n",
      "epoch: 9 step: 32, loss is 0.6043879389762878\n",
      "epoch: 9 step: 33, loss is 0.5408286452293396\n",
      "epoch: 9 step: 34, loss is 0.5860438346862793\n",
      "epoch: 9 step: 35, loss is 0.5453611016273499\n",
      "epoch: 9 step: 36, loss is 0.5849680304527283\n",
      "epoch: 9 step: 37, loss is 0.564490020275116\n",
      "epoch: 9 step: 38, loss is 0.5870996713638306\n",
      "epoch: 9 step: 39, loss is 0.5687922835350037\n",
      "epoch: 9 step: 40, loss is 0.5354545712471008\n",
      "epoch: 9 step: 41, loss is 0.5472893714904785\n",
      "epoch: 9 step: 42, loss is 0.5591422319412231\n",
      "epoch: 9 step: 43, loss is 0.5451415777206421\n",
      "epoch: 9 step: 44, loss is 0.5752399563789368\n",
      "epoch: 9 step: 45, loss is 0.5526967644691467\n",
      "epoch: 9 step: 46, loss is 0.551581621170044\n",
      "epoch: 9 step: 47, loss is 0.5410712957382202\n",
      "epoch: 9 step: 48, loss is 0.5817179679870605\n",
      "epoch: 9 step: 49, loss is 0.5687982439994812\n",
      "epoch: 9 step: 50, loss is 0.5569484233856201\n",
      "epoch: 9 step: 51, loss is 0.5741900205612183\n",
      "epoch: 9 step: 52, loss is 0.569061815738678\n",
      "epoch: 9 step: 53, loss is 0.5698713660240173\n",
      "epoch: 9 step: 54, loss is 0.5602056384086609\n",
      "epoch: 9 step: 55, loss is 0.5924752354621887\n",
      "epoch: 9 step: 56, loss is 0.5709419250488281\n",
      "epoch: 9 step: 57, loss is 0.5429942607879639\n",
      "epoch: 9 step: 58, loss is 0.5591502785682678\n",
      "epoch: 9 step: 59, loss is 0.5421774387359619\n",
      "epoch: 9 step: 60, loss is 0.5440812110900879\n",
      "epoch: 9 step: 61, loss is 0.5483729839324951\n",
      "epoch: 9 step: 62, loss is 0.5730912089347839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 63, loss is 0.5569601655006409\n",
      "epoch: 9 step: 64, loss is 0.552660346031189\n",
      "epoch: 9 step: 65, loss is 0.5763317942619324\n",
      "epoch: 9 step: 66, loss is 0.5515755414962769\n",
      "epoch: 9 step: 67, loss is 0.5731020569801331\n",
      "epoch: 9 step: 68, loss is 0.5655638575553894\n",
      "epoch: 9 step: 69, loss is 0.5946428179740906\n",
      "epoch: 9 step: 70, loss is 0.5332779288291931\n",
      "epoch: 9 step: 71, loss is 0.5591080188751221\n",
      "epoch: 9 step: 72, loss is 0.5730929374694824\n",
      "epoch: 9 step: 73, loss is 0.5279038548469543\n",
      "epoch: 9 step: 74, loss is 0.5752511024475098\n",
      "epoch: 9 step: 75, loss is 0.5903303027153015\n",
      "epoch: 9 step: 76, loss is 0.5515754818916321\n",
      "epoch: 9 step: 77, loss is 0.5765879154205322\n",
      "epoch: 9 step: 78, loss is 0.5548292398452759\n",
      "epoch: 9 step: 79, loss is 0.5666365623474121\n",
      "epoch: 9 step: 80, loss is 0.5666417479515076\n",
      "epoch: 9 step: 81, loss is 0.5666701793670654\n",
      "epoch: 9 step: 82, loss is 0.5548167824745178\n",
      "epoch: 9 step: 83, loss is 0.5744000673294067\n",
      "epoch: 9 step: 84, loss is 0.5515950918197632\n",
      "epoch: 9 step: 85, loss is 0.5301280617713928\n",
      "epoch: 9 step: 86, loss is 0.5741505026817322\n",
      "epoch: 9 step: 87, loss is 0.5591168403625488\n",
      "epoch: 9 step: 88, loss is 0.5430394411087036\n",
      "epoch: 9 step: 89, loss is 0.5849040746688843\n",
      "epoch: 9 step: 90, loss is 0.5655553340911865\n",
      "epoch: 9 step: 91, loss is 0.5709353089332581\n",
      "epoch: 9 step: 92, loss is 0.5569604635238647\n",
      "epoch: 9 step: 93, loss is 0.544069230556488\n",
      "epoch: 9 step: 94, loss is 0.5698766708374023\n",
      "epoch: 9 step: 95, loss is 0.5505102872848511\n",
      "epoch: 9 step: 96, loss is 0.5763153433799744\n",
      "epoch: 9 step: 97, loss is 0.552662193775177\n",
      "epoch: 9 step: 98, loss is 0.5357097387313843\n",
      "epoch: 9 step: 99, loss is 0.5408166646957397\n",
      "epoch: 9 step: 100, loss is 0.5860274434089661\n",
      "epoch: 9 step: 101, loss is 0.5644920468330383\n",
      "epoch: 9 step: 102, loss is 0.5798219442367554\n",
      "epoch: 9 step: 103, loss is 0.5655710101127625\n",
      "epoch: 9 step: 104, loss is 0.5617696642875671\n",
      "epoch: 9 step: 105, loss is 0.5763429999351501\n",
      "epoch: 9 step: 106, loss is 0.5612603425979614\n",
      "epoch: 9 step: 107, loss is 0.5687978267669678\n",
      "epoch: 9 step: 108, loss is 0.5655707120895386\n",
      "epoch: 9 step: 109, loss is 0.5644842982292175\n",
      "epoch: 9 step: 110, loss is 0.5516358017921448\n",
      "epoch: 9 step: 111, loss is 0.5558837652206421\n",
      "epoch: 9 step: 112, loss is 0.5580345988273621\n",
      "epoch: 9 step: 113, loss is 0.5612597465515137\n",
      "epoch: 9 step: 114, loss is 0.5655577182769775\n",
      "epoch: 9 step: 115, loss is 0.5634108185768127\n",
      "epoch: 9 step: 116, loss is 0.5558938384056091\n",
      "epoch: 9 step: 117, loss is 0.5257944464683533\n",
      "epoch: 9 step: 118, loss is 0.5752468109130859\n",
      "epoch: 9 step: 119, loss is 0.5720129013061523\n",
      "epoch: 9 step: 120, loss is 0.5569548010826111\n",
      "epoch: 9 step: 121, loss is 0.5580321550369263\n",
      "epoch: 9 step: 122, loss is 0.5518136024475098\n",
      "epoch: 9 step: 123, loss is 0.5763371586799622\n",
      "epoch: 9 step: 124, loss is 0.5957189798355103\n",
      "epoch: 9 step: 125, loss is 0.5418835878372192\n",
      "epoch: 9 step: 126, loss is 0.5827828049659729\n",
      "epoch: 9 step: 127, loss is 0.5289849042892456\n",
      "epoch: 9 step: 128, loss is 0.5375905632972717\n",
      "epoch: 9 step: 129, loss is 0.5547981262207031\n",
      "epoch: 9 step: 130, loss is 0.5828016996383667\n",
      "epoch: 9 step: 131, loss is 0.5528997182846069\n",
      "epoch: 9 step: 132, loss is 0.5580211877822876\n",
      "epoch: 9 step: 133, loss is 0.5807201862335205\n",
      "epoch: 9 step: 134, loss is 0.5677310824394226\n",
      "epoch: 9 step: 135, loss is 0.5418573021888733\n",
      "epoch: 9 step: 136, loss is 0.5817443132400513\n",
      "epoch: 9 step: 137, loss is 0.5828182101249695\n",
      "epoch: 9 step: 138, loss is 0.5666465759277344\n",
      "epoch: 9 step: 139, loss is 0.5687944889068604\n",
      "epoch: 9 step: 140, loss is 0.5733441114425659\n",
      "epoch: 9 step: 141, loss is 0.5475696325302124\n",
      "epoch: 9 step: 142, loss is 0.5720065236091614\n",
      "epoch: 9 step: 143, loss is 0.5623356699943542\n",
      "epoch: 9 step: 144, loss is 0.5644868016242981\n",
      "epoch: 9 step: 145, loss is 0.563405454158783\n",
      "epoch: 9 step: 146, loss is 0.5634065866470337\n",
      "epoch: 9 step: 147, loss is 0.5441030859947205\n",
      "epoch: 9 step: 148, loss is 0.5644800066947937\n",
      "epoch: 9 step: 149, loss is 0.5655663013458252\n",
      "epoch: 9 step: 150, loss is 0.5505416393280029\n",
      "epoch: 9 step: 151, loss is 0.564478874206543\n",
      "epoch: 9 step: 152, loss is 0.5323156118392944\n",
      "epoch: 9 step: 153, loss is 0.5623410940170288\n",
      "epoch: 9 step: 154, loss is 0.5655547976493835\n",
      "epoch: 9 step: 155, loss is 0.5644848942756653\n",
      "epoch: 9 step: 156, loss is 0.5881060361862183\n",
      "epoch: 9 step: 157, loss is 0.5644773244857788\n",
      "epoch: 9 step: 158, loss is 0.5709202885627747\n",
      "epoch: 9 step: 159, loss is 0.5539931058883667\n",
      "epoch: 9 step: 160, loss is 0.5558950901031494\n",
      "epoch: 9 step: 161, loss is 0.5741494297981262\n",
      "epoch: 9 step: 162, loss is 0.5537523627281189\n",
      "epoch: 9 step: 163, loss is 0.5559307932853699\n",
      "epoch: 9 step: 164, loss is 0.546241819858551\n",
      "epoch: 9 step: 165, loss is 0.5741406083106995\n",
      "epoch: 9 step: 166, loss is 0.5462344884872437\n",
      "epoch: 9 step: 167, loss is 0.5314634442329407\n",
      "epoch: 9 step: 168, loss is 0.5580350756645203\n",
      "epoch: 9 step: 169, loss is 0.5827692151069641\n",
      "epoch: 9 step: 170, loss is 0.5677129030227661\n",
      "epoch: 10 step: 1, loss is 0.5375849008560181\n",
      "epoch: 10 step: 2, loss is 0.6150955557823181\n",
      "epoch: 10 step: 3, loss is 0.5526490211486816\n",
      "epoch: 10 step: 4, loss is 0.5795567035675049\n",
      "epoch: 10 step: 5, loss is 0.5612624287605286\n",
      "epoch: 10 step: 6, loss is 0.5634081959724426\n",
      "epoch: 10 step: 7, loss is 0.5625891089439392\n",
      "epoch: 10 step: 8, loss is 0.5580393671989441\n",
      "epoch: 10 step: 9, loss is 0.5526838898658752\n",
      "epoch: 10 step: 10, loss is 0.5677381157875061\n",
      "epoch: 10 step: 11, loss is 0.5655567646026611\n",
      "epoch: 10 step: 12, loss is 0.5410916805267334\n",
      "epoch: 10 step: 13, loss is 0.540855884552002\n",
      "epoch: 10 step: 14, loss is 0.5612589120864868\n",
      "epoch: 10 step: 15, loss is 0.5687892436981201\n",
      "epoch: 10 step: 16, loss is 0.5429785251617432\n",
      "epoch: 10 step: 17, loss is 0.5709425210952759\n",
      "epoch: 10 step: 18, loss is 0.5677212476730347\n",
      "epoch: 10 step: 19, loss is 0.5515703558921814\n",
      "epoch: 10 step: 20, loss is 0.5698739290237427\n",
      "epoch: 10 step: 21, loss is 0.5655670166015625\n",
      "epoch: 10 step: 22, loss is 0.5763457417488098\n",
      "epoch: 10 step: 23, loss is 0.5601841807365417\n",
      "epoch: 10 step: 24, loss is 0.5765805840492249\n",
      "epoch: 10 step: 25, loss is 0.5935811400413513\n",
      "epoch: 10 step: 26, loss is 0.5386645197868347\n",
      "epoch: 10 step: 27, loss is 0.5720133781433105\n",
      "epoch: 10 step: 28, loss is 0.5784619450569153\n",
      "epoch: 10 step: 29, loss is 0.5419321060180664\n",
      "epoch: 10 step: 30, loss is 0.5451540946960449\n",
      "epoch: 10 step: 31, loss is 0.5462291240692139\n",
      "epoch: 10 step: 32, loss is 0.5679879784584045\n",
      "epoch: 10 step: 33, loss is 0.5526680946350098\n",
      "epoch: 10 step: 34, loss is 0.550514817237854\n",
      "epoch: 10 step: 35, loss is 0.5376108884811401\n",
      "epoch: 10 step: 36, loss is 0.5580294728279114\n",
      "epoch: 10 step: 37, loss is 0.5300332903862\n",
      "epoch: 10 step: 38, loss is 0.5979188084602356\n",
      "epoch: 10 step: 39, loss is 0.5547902584075928\n",
      "epoch: 10 step: 40, loss is 0.5742446780204773\n",
      "epoch: 10 step: 41, loss is 0.5515468120574951\n",
      "epoch: 10 step: 42, loss is 0.563417911529541\n",
      "epoch: 10 step: 43, loss is 0.5582842230796814\n",
      "epoch: 10 step: 44, loss is 0.5515315532684326\n",
      "epoch: 10 step: 45, loss is 0.5807217359542847\n",
      "epoch: 10 step: 46, loss is 0.5807265639305115\n",
      "epoch: 10 step: 47, loss is 0.5601803064346313\n",
      "epoch: 10 step: 48, loss is 0.549369752407074\n",
      "epoch: 10 step: 49, loss is 0.5504506826400757\n",
      "epoch: 10 step: 50, loss is 0.5785512328147888\n",
      "epoch: 10 step: 51, loss is 0.5582582950592041\n",
      "epoch: 10 step: 52, loss is 0.5493795275688171\n",
      "epoch: 10 step: 53, loss is 0.5580211877822876\n",
      "epoch: 10 step: 54, loss is 0.5655794739723206\n",
      "epoch: 10 step: 55, loss is 0.5604318976402283\n",
      "epoch: 10 step: 56, loss is 0.5753155946731567\n",
      "epoch: 10 step: 57, loss is 0.5580206513404846\n",
      "epoch: 10 step: 58, loss is 0.5591306090354919\n",
      "epoch: 10 step: 59, loss is 0.5820234417915344\n",
      "epoch: 10 step: 60, loss is 0.5493946075439453\n",
      "epoch: 10 step: 61, loss is 0.5688196420669556\n",
      "epoch: 10 step: 62, loss is 0.5385969877243042\n",
      "epoch: 10 step: 63, loss is 0.5644977688789368\n",
      "epoch: 10 step: 64, loss is 0.5828526616096497\n",
      "epoch: 10 step: 65, loss is 0.566658079624176\n",
      "epoch: 10 step: 66, loss is 0.576360285282135\n",
      "epoch: 10 step: 67, loss is 0.560180127620697\n",
      "epoch: 10 step: 68, loss is 0.5354033708572388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 69, loss is 0.5709591507911682\n",
      "epoch: 10 step: 70, loss is 0.5558795928955078\n",
      "epoch: 10 step: 71, loss is 0.5494070649147034\n",
      "epoch: 10 step: 72, loss is 0.5817216038703918\n",
      "epoch: 10 step: 73, loss is 0.5817257165908813\n",
      "epoch: 10 step: 74, loss is 0.5418823957443237\n",
      "epoch: 10 step: 75, loss is 0.5698694586753845\n",
      "epoch: 10 step: 76, loss is 0.5795499086380005\n",
      "epoch: 10 step: 77, loss is 0.5410915613174438\n",
      "epoch: 10 step: 78, loss is 0.5612611174583435\n",
      "epoch: 10 step: 79, loss is 0.5472856163978577\n",
      "epoch: 10 step: 80, loss is 0.5711707472801208\n",
      "epoch: 10 step: 81, loss is 0.5537360310554504\n",
      "epoch: 10 step: 82, loss is 0.5816845297813416\n",
      "epoch: 10 step: 83, loss is 0.5644840598106384\n",
      "epoch: 10 step: 84, loss is 0.5570064783096313\n",
      "epoch: 10 step: 85, loss is 0.5440744757652283\n",
      "epoch: 10 step: 86, loss is 0.5902819633483887\n",
      "epoch: 10 step: 87, loss is 0.5462229251861572\n",
      "epoch: 10 step: 88, loss is 0.5623319745063782\n",
      "epoch: 10 step: 89, loss is 0.5623379349708557\n",
      "epoch: 10 step: 90, loss is 0.5580405592918396\n",
      "epoch: 10 step: 91, loss is 0.5614994764328003\n",
      "epoch: 10 step: 92, loss is 0.5569649338722229\n",
      "epoch: 10 step: 93, loss is 0.5666365623474121\n",
      "epoch: 10 step: 94, loss is 0.5644901394844055\n",
      "epoch: 10 step: 95, loss is 0.5558942556381226\n",
      "epoch: 10 step: 96, loss is 0.5486173629760742\n",
      "epoch: 10 step: 97, loss is 0.5612642765045166\n",
      "epoch: 10 step: 98, loss is 0.5472816824913025\n",
      "epoch: 10 step: 99, loss is 0.5741718411445618\n",
      "epoch: 10 step: 100, loss is 0.553726315498352\n",
      "epoch: 10 step: 101, loss is 0.5559069514274597\n",
      "epoch: 10 step: 102, loss is 0.5795676708221436\n",
      "epoch: 10 step: 103, loss is 0.5526461005210876\n",
      "epoch: 10 step: 104, loss is 0.5612609386444092\n",
      "epoch: 10 step: 105, loss is 0.5774191617965698\n",
      "epoch: 10 step: 106, loss is 0.5505189895629883\n",
      "epoch: 10 step: 107, loss is 0.5354025959968567\n",
      "epoch: 10 step: 108, loss is 0.5461646914482117\n",
      "epoch: 10 step: 109, loss is 0.5569496154785156\n",
      "epoch: 10 step: 110, loss is 0.5755303502082825\n",
      "epoch: 10 step: 111, loss is 0.5482993125915527\n",
      "epoch: 10 step: 112, loss is 0.5688230395317078\n",
      "epoch: 10 step: 113, loss is 0.5709897875785828\n",
      "epoch: 10 step: 114, loss is 0.6034286618232727\n",
      "epoch: 10 step: 115, loss is 0.5407270789146423\n",
      "epoch: 10 step: 116, loss is 0.5615265965461731\n",
      "epoch: 10 step: 117, loss is 0.572060763835907\n",
      "epoch: 10 step: 118, loss is 0.5752931833267212\n",
      "epoch: 10 step: 119, loss is 0.5504652857780457\n",
      "epoch: 10 step: 120, loss is 0.5267617106437683\n",
      "epoch: 10 step: 121, loss is 0.5461493134498596\n",
      "epoch: 10 step: 122, loss is 0.5958125591278076\n",
      "epoch: 10 step: 123, loss is 0.5526242852210999\n",
      "epoch: 10 step: 124, loss is 0.5591039061546326\n",
      "epoch: 10 step: 125, loss is 0.5569424629211426\n",
      "epoch: 10 step: 126, loss is 0.5407513380050659\n",
      "epoch: 10 step: 127, loss is 0.5915082693099976\n",
      "epoch: 10 step: 128, loss is 0.5558596253395081\n",
      "epoch: 10 step: 129, loss is 0.5763810873031616\n",
      "epoch: 10 step: 130, loss is 0.558021068572998\n",
      "epoch: 10 step: 131, loss is 0.5634207725524902\n",
      "epoch: 10 step: 132, loss is 0.560183584690094\n",
      "epoch: 10 step: 133, loss is 0.5785316824913025\n",
      "epoch: 10 step: 134, loss is 0.556948184967041\n",
      "epoch: 10 step: 135, loss is 0.5647488832473755\n",
      "epoch: 10 step: 136, loss is 0.5668728947639465\n",
      "epoch: 10 step: 137, loss is 0.5623354911804199\n",
      "epoch: 10 step: 138, loss is 0.5612614750862122\n",
      "epoch: 10 step: 139, loss is 0.556952714920044\n",
      "epoch: 10 step: 140, loss is 0.5451130867004395\n",
      "epoch: 10 step: 141, loss is 0.5494192838668823\n",
      "epoch: 10 step: 142, loss is 0.5580355525016785\n",
      "epoch: 10 step: 143, loss is 0.5688022971153259\n",
      "epoch: 10 step: 144, loss is 0.5644930005073547\n",
      "epoch: 10 step: 145, loss is 0.5862873196601868\n",
      "epoch: 10 step: 146, loss is 0.5515936613082886\n",
      "epoch: 10 step: 147, loss is 0.5591100454330444\n",
      "epoch: 10 step: 148, loss is 0.5386465191841125\n",
      "epoch: 10 step: 149, loss is 0.558268666267395\n",
      "epoch: 10 step: 150, loss is 0.5860428214073181\n",
      "epoch: 10 step: 151, loss is 0.5375635623931885\n",
      "epoch: 10 step: 152, loss is 0.5741863250732422\n",
      "epoch: 10 step: 153, loss is 0.5838928818702698\n",
      "epoch: 10 step: 154, loss is 0.5537217259407043\n",
      "epoch: 10 step: 155, loss is 0.5453293919563293\n",
      "epoch: 10 step: 156, loss is 0.5537197589874268\n",
      "epoch: 10 step: 157, loss is 0.5483337640762329\n",
      "epoch: 10 step: 158, loss is 0.53431236743927\n",
      "epoch: 10 step: 159, loss is 0.5504772067070007\n",
      "epoch: 10 step: 160, loss is 0.567740797996521\n",
      "epoch: 10 step: 161, loss is 0.5688288807868958\n",
      "epoch: 10 step: 162, loss is 0.5699141621589661\n",
      "epoch: 10 step: 163, loss is 0.5504480004310608\n",
      "epoch: 10 step: 164, loss is 0.5734142661094666\n",
      "epoch: 10 step: 165, loss is 0.5471948385238647\n",
      "epoch: 10 step: 166, loss is 0.554795503616333\n",
      "epoch: 10 step: 167, loss is 0.567762553691864\n",
      "epoch: 10 step: 168, loss is 0.5850969552993774\n",
      "epoch: 10 step: 169, loss is 0.5829330682754517\n",
      "epoch: 10 step: 170, loss is 0.5829175114631653\n",
      "============== Starting Testing ==============\n",
      "============== Accuracy:{'Accuracy': 0.75} ==============\n"
     ]
    }
   ],
   "source": [
    "#define the optimizer\n",
    "net_opt = nn.Momentum(network.trainable_params(), lr, momentum) \n",
    "\n",
    "# set parameters of check point\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10) \n",
    "\n",
    "# apply parameters of check point\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck) \n",
    "\n",
    "# group layers into an object with training and evaluation features\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "\n",
    "# Perform training\n",
    "train_net(args, model, epoch_size, dataset_path, repeat_size, ckpoint_cb, dataset_sink_mode, batch_size = batch_size)\n",
    "test_net(args, network, model, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdeada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8270d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627996d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f43a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb26ebfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ca9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
